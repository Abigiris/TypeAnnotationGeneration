# Effective Evaluation

The data and script can be used to replicate our results in RQ1 and RQ3, as well as for future tool evaluations.

### Evaluated Data

- ground_truth/: Type annotations provided by project developers. We extract the data from source codes by AST analysis.
- pytype_results/: Type annotations generated by [Pytype](https://github.com/google/pytype "Pytype"). As the raw results are stub files (.pyi), we also extract type information by AST analysis.
- type4py\_results/: Type annotations generated by [Type4Py](https://github.com/saltudelft/type4py "Type4Py"). This model outputs JSON files, and we adjust them to the same format as the results in ground_truth/.
- hityper_results/: Type annotations generated by [HiTyper](https://github.com/JohnnyPeng18/HiTyper "HiTyper"). This tool outputs JSON files, and we also make a few adjustments.


**Attentionï¼š** Keep the result format consistent with the groun truth format, especially file paths, function names, and argument names. We use JSON files to store the results in our experiment, and you can modify *evaluation_annotations.py* to suit your own data format.


### Usage

Run this command to evaluate the tool-generated results:

    python3 evaluate_annotations.py <tool_results_dir> <ground_truth_dir> <output_path>

For example, to evaluate the effectiveness of Pytype (the default is Top-1 with all type categories):

    python3 evaluate_annotations.py pytype_results/ ground_truth/ pytype_top1.json

There are two settings in *evaluation_annotations.py* can be modified to complete different categories of evaluation:

    TOP_N = 1
	TYPE_LIMIT = [TypeCategory.Elementary, TypeCategory.Parametric, TypeCategory.Union,
             	 TypeCategory.Dynamic, TypeCategory.Variable, TypeCategory.UserDefined]

{
    "implementations-dqn-master/train_eval.py": {
        "get_config": {
            "name": "get_config",
            "location": 74,
            "return": [],
            "arguments": {}
        },
        "train_eval": {
            "name": "train_eval",
            "location": 245,
            "return": [],
            "arguments": {
                "dqn_agent": [],
                "replay_buffer": [],
                "env": [],
                "eval_env": [],
                "device": [],
                "logger": [],
                "CONFIG": []
            }
        },
        "main": {
            "name": "main",
            "location": 416,
            "return": [],
            "arguments": {}
        }
    },
    "implementations-dqn-master/train_eval_atari.py": {
        "main": {
            "name": "main",
            "location": 67,
            "return": [],
            "arguments": {}
        }
    },
    "implementations-dqn-master/dqn/agents.py": {
        "DQNAgent.__init__": {
            "name": "__init__",
            "location": 15,
            "return": [],
            "arguments": {
                "self": [],
                "env": [
                    "gym.Env"
                ],
                "q_net": [
                    "nn.Module"
                ],
                "optimizer": [
                    "optim.Optimizer"
                ],
                "device": [
                    "torch.device"
                ]
            }
        },
        "DQNAgent.select_action": {
            "name": "select_action",
            "location": 45,
            "return": [
                "int"
            ],
            "arguments": {
                "self": [],
                "obs": [
                    "torch.Tensor"
                ],
                "epsilon": [
                    "float"
                ]
            }
        },
        "DQNAgent.train": {
            "name": "train",
            "location": 71,
            "return": [
                "float"
            ],
            "arguments": {
                "self": [],
                "experiences": [
                    "List[Tuple[(np.array, int, float, np.array, bool)]]"
                ],
                "discount": [
                    "float"
                ]
            }
        },
        "DQNAgent.update_target_q_net": {
            "name": "update_target_q_net",
            "location": 115,
            "return": [],
            "arguments": {
                "self": []
            }
        }
    },
    "implementations-dqn-master/dqn/networks.py": {
        "QNetwork.__init__": {
            "name": "__init__",
            "location": 10,
            "return": [],
            "arguments": {
                "self": [],
                "in_dim": [
                    "int"
                ],
                "out_dim": [
                    "int"
                ]
            }
        },
        "QNetwork.forward": {
            "name": "forward",
            "location": 38,
            "return": [
                "torch.Tensor"
            ],
            "arguments": {
                "self": [],
                "x": [
                    "torch.Tensor"
                ]
            }
        },
        "AtariQNetwork.__init__": {
            "name": "__init__",
            "location": 56,
            "return": [],
            "arguments": {
                "self": [],
                "in_dim": [
                    "int"
                ],
                "out_dim": [
                    "int"
                ]
            }
        },
        "AtariQNetwork.forward": {
            "name": "forward",
            "location": 88,
            "return": [
                "torch.Tensor"
            ],
            "arguments": {
                "self": [],
                "x": [
                    "torch.Tensor"
                ]
            }
        },
        "QNetwork.__init__.weights_init": {
            "name": "weights_init",
            "location": 31,
            "return": [],
            "arguments": {
                "m": []
            }
        },
        "AtariQNetwork.__init__.weights_init": {
            "name": "weights_init",
            "location": 80,
            "return": [],
            "arguments": {
                "m": []
            }
        }
    },
    "implementations-dqn-master/dqn/replays.py": {
        "NATUREDQN_ATARI_PREPROCESS_BATCH": {
            "name": "NATUREDQN_ATARI_PREPROCESS_BATCH",
            "location": 14,
            "return": [],
            "arguments": {
                "o": [],
                "a": [],
                "r": [],
                "n": [],
                "d": []
            }
        },
        "NORMALIZE_OBSERVATION": {
            "name": "NORMALIZE_OBSERVATION",
            "location": 22,
            "return": [],
            "arguments": {
                "obs_b": [],
                "action_b": [],
                "rew_b": [],
                "next_obs_b": [],
                "done_b": []
            }
        },
        "CLIP_REWARD": {
            "name": "CLIP_REWARD",
            "location": 30,
            "return": [],
            "arguments": {
                "obs_b": [],
                "action_b": [],
                "rew_b": [],
                "next_obs_b": [],
                "done_b": []
            }
        },
        "FIXED_MAGNITUDE_REWARD": {
            "name": "FIXED_MAGNITUDE_REWARD",
            "location": 36,
            "return": [],
            "arguments": {
                "obs_b": [],
                "action_b": [],
                "rew_b": [],
                "next_obs_b": [],
                "done_b": []
            }
        },
        "ReplayBuffer.__init__": {
            "name": "__init__",
            "location": 42,
            "return": [],
            "arguments": {
                "self": [],
                "maxlen": [
                    "int"
                ],
                "device": [
                    "torch.device"
                ],
                "preprocess_batch": [
                    "Callable"
                ]
            }
        },
        "ReplayBuffer.__len__": {
            "name": "__len__",
            "location": 61,
            "return": [],
            "arguments": {
                "self": []
            }
        },
        "ReplayBuffer.append": {
            "name": "append",
            "location": 64,
            "return": [],
            "arguments": {
                "self": [],
                "transition": [
                    "Transition"
                ]
            }
        },
        "ReplayBuffer.get_numpy_batch": {
            "name": "get_numpy_batch",
            "location": 76,
            "return": [
                "Tuple[(np.array, np.array, np.array, np.array, np.array)]"
            ],
            "arguments": {
                "self": [],
                "batch_size": [
                    "int"
                ]
            }
        },
        "ReplayBuffer.get_torch_batch": {
            "name": "get_torch_batch",
            "location": 118,
            "return": [
                "Tuple[(torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor)]"
            ],
            "arguments": {
                "self": [],
                "batch_size": [
                    "int"
                ]
            }
        },
        "CircularReplayBuffer.__init__": {
            "name": "__init__",
            "location": 153,
            "return": [],
            "arguments": {
                "self": [],
                "env": [
                    "gym.Env"
                ],
                "maxlen": [
                    "int"
                ],
                "device": [
                    "torch.device"
                ],
                "preprocess_batch": [
                    "Callable"
                ]
            }
        },
        "CircularReplayBuffer.__len__": {
            "name": "__len__",
            "location": 197,
            "return": [],
            "arguments": {
                "self": []
            }
        },
        "CircularReplayBuffer.append": {
            "name": "append",
            "location": 200,
            "return": [],
            "arguments": {
                "self": [],
                "transition": [
                    "Transition"
                ]
            }
        },
        "CircularReplayBuffer.get_numpy_batch": {
            "name": "get_numpy_batch",
            "location": 215,
            "return": [
                "Tuple[(np.array, np.array, np.array, np.array, np.array)]"
            ],
            "arguments": {
                "self": [],
                "batch_size": [
                    "int"
                ]
            }
        },
        "CircularReplayBuffer.get_torch_batch": {
            "name": "get_torch_batch",
            "location": 259,
            "return": [
                "Tuple[(torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor)]"
            ],
            "arguments": {
                "self": [],
                "batch_size": [
                    "int"
                ]
            }
        }
    },
    "implementations-dqn-master/dqn/__init__.py": {},
    "implementations-dqn-master/environments/atari_preprocessing.py": {
        "AtariPreprocessing.__init__": {
            "name": "__init__",
            "location": 36,
            "return": [],
            "arguments": {
                "self": [],
                "env": [],
                "noop_max": [],
                "frame_skip": [],
                "screen_size": [],
                "terminal_on_life_loss": [],
                "grayscale_obs": []
            }
        },
        "AtariPreprocessing.step": {
            "name": "step",
            "location": 82,
            "return": [],
            "arguments": {
                "self": [],
                "action": []
            }
        },
        "AtariPreprocessing.reset": {
            "name": "reset",
            "location": 109,
            "return": [],
            "arguments": {
                "self": []
            }
        },
        "AtariPreprocessing._get_obs": {
            "name": "_get_obs",
            "location": 135,
            "return": [],
            "arguments": {
                "self": []
            }
        }
    },
    "implementations-dqn-master/environments/atari_wrappers.py": {
        "make_atari": {
            "name": "make_atari",
            "location": 308,
            "return": [],
            "arguments": {
                "env_id": [],
                "max_episode_steps": []
            }
        },
        "wrap_deepmind": {
            "name": "wrap_deepmind",
            "location": 318,
            "return": [],
            "arguments": {
                "env": [],
                "episode_life": [],
                "clip_rewards": [],
                "frame_stack": [],
                "scale": []
            }
        },
        "TimeLimit.__init__": {
            "name": "__init__",
            "location": 23,
            "return": [],
            "arguments": {
                "self": [],
                "env": [],
                "max_episode_steps": []
            }
        },
        "TimeLimit.step": {
            "name": "step",
            "location": 28,
            "return": [],
            "arguments": {
                "self": [],
                "ac": []
            }
        },
        "TimeLimit.reset": {
            "name": "reset",
            "location": 36,
            "return": [],
            "arguments": {
                "self": []
            }
        },
        "NoopResetEnv.__init__": {
            "name": "__init__",
            "location": 42,
            "return": [],
            "arguments": {
                "self": [],
                "env": [],
                "noop_max": []
            }
        },
        "NoopResetEnv.reset": {
            "name": "reset",
            "location": 52,
            "return": [],
            "arguments": {
                "self": []
            }
        },
        "NoopResetEnv.step": {
            "name": "step",
            "location": 69,
            "return": [],
            "arguments": {
                "self": [],
                "ac": []
            }
        },
        "FireResetEnv.__init__": {
            "name": "__init__",
            "location": 74,
            "return": [],
            "arguments": {
                "self": [],
                "env": []
            }
        },
        "FireResetEnv.reset": {
            "name": "reset",
            "location": 80,
            "return": [],
            "arguments": {
                "self": []
            }
        },
        "FireResetEnv.step": {
            "name": "step",
            "location": 90,
            "return": [],
            "arguments": {
                "self": [],
                "ac": []
            }
        },
        "EpisodicLifeEnv.__init__": {
            "name": "__init__",
            "location": 95,
            "return": [],
            "arguments": {
                "self": [],
                "env": []
            }
        },
        "EpisodicLifeEnv.step": {
            "name": "step",
            "location": 103,
            "return": [],
            "arguments": {
                "self": [],
                "action": []
            }
        },
        "EpisodicLifeEnv.reset": {
            "name": "reset",
            "location": 117,
            "return": [],
            "arguments": {
                "self": []
            }
        },
        "MaxAndSkipEnv.__init__": {
            "name": "__init__",
            "location": 132,
            "return": [],
            "arguments": {
                "self": [],
                "env": [],
                "skip": []
            }
        },
        "MaxAndSkipEnv.step": {
            "name": "step",
            "location": 139,
            "return": [],
            "arguments": {
                "self": [],
                "action": []
            }
        },
        "MaxAndSkipEnv.reset": {
            "name": "reset",
            "location": 158,
            "return": [],
            "arguments": {
                "self": []
            }
        },
        "ClipRewardEnv.__init__": {
            "name": "__init__",
            "location": 163,
            "return": [],
            "arguments": {
                "self": [],
                "env": []
            }
        },
        "ClipRewardEnv.reward": {
            "name": "reward",
            "location": 166,
            "return": [],
            "arguments": {
                "self": [],
                "reward": []
            }
        },
        "WarpFrame.__init__": {
            "name": "__init__",
            "location": 172,
            "return": [],
            "arguments": {
                "self": [],
                "env": [],
                "width": [],
                "height": [],
                "grayscale": [],
                "dict_space_key": []
            }
        },
        "WarpFrame.observation": {
            "name": "observation",
            "location": 202,
            "return": [],
            "arguments": {
                "self": [],
                "obs": []
            }
        },
        "FrameStack.__init__": {
            "name": "__init__",
            "location": 225,
            "return": [],
            "arguments": {
                "self": [],
                "env": [],
                "k": []
            }
        },
        "FrameStack.reset": {
            "name": "reset",
            "location": 243,
            "return": [],
            "arguments": {
                "self": []
            }
        },
        "FrameStack.step": {
            "name": "step",
            "location": 249,
            "return": [],
            "arguments": {
                "self": [],
                "action": []
            }
        },
        "FrameStack._get_ob": {
            "name": "_get_ob",
            "location": 254,
            "return": [],
            "arguments": {
                "self": []
            }
        },
        "ScaledFloatFrame.__init__": {
            "name": "__init__",
            "location": 260,
            "return": [],
            "arguments": {
                "self": [],
                "env": []
            }
        },
        "ScaledFloatFrame.observation": {
            "name": "observation",
            "location": 266,
            "return": [],
            "arguments": {
                "self": [],
                "observation": []
            }
        },
        "LazyFrames.__init__": {
            "name": "__init__",
            "location": 273,
            "return": [],
            "arguments": {
                "self": [],
                "frames": []
            }
        },
        "LazyFrames._force": {
            "name": "_force",
            "location": 282,
            "return": [],
            "arguments": {
                "self": []
            }
        },
        "LazyFrames.__array__": {
            "name": "__array__",
            "location": 288,
            "return": [],
            "arguments": {
                "self": [],
                "dtype": []
            }
        },
        "LazyFrames.__len__": {
            "name": "__len__",
            "location": 294,
            "return": [],
            "arguments": {
                "self": []
            }
        },
        "LazyFrames.__getitem__": {
            "name": "__getitem__",
            "location": 297,
            "return": [],
            "arguments": {
                "self": [],
                "i": []
            }
        },
        "LazyFrames.count": {
            "name": "count",
            "location": 300,
            "return": [],
            "arguments": {
                "self": []
            }
        },
        "LazyFrames.frame": {
            "name": "frame",
            "location": 304,
            "return": [],
            "arguments": {
                "self": [],
                "i": []
            }
        }
    },
    "implementations-dqn-master/environments/frame_stack.py": {
        "FrameStack.__init__": {
            "name": "__init__",
            "location": 15,
            "return": [],
            "arguments": {
                "self": [],
                "env": [],
                "stack_size": []
            }
        },
        "FrameStack.__getattr__": {
            "name": "__getattr__",
            "location": 27,
            "return": [],
            "arguments": {
                "self": [],
                "name": []
            }
        },
        "FrameStack._generate_observation": {
            "name": "_generate_observation",
            "location": 31,
            "return": [],
            "arguments": {
                "self": []
            }
        },
        "FrameStack.reset": {
            "name": "reset",
            "location": 34,
            "return": [],
            "arguments": {
                "self": []
            }
        },
        "FrameStack.step": {
            "name": "step",
            "location": 40,
            "return": [],
            "arguments": {
                "self": [],
                "action": []
            }
        }
    },
    "implementations-dqn-master/environments/__init__.py": {},
    "implementations-dqn-master/tests/__init__.py": {}
}
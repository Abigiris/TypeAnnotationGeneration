{
    "my_agents-master/setup.py": {},
    "my_agents-master/my_agents/comparison.py": {},
    "my_agents-master/my_agents/evaluate.py": {},
    "my_agents-master/my_agents/main.py": {},
    "my_agents-master/my_agents/vectorize_test.py": {},
    "my_agents-master/my_agents/agents/ddqn_agent.py": {
        "build_dense_network": {
            "name": "build_dense_network",
            "location": 24,
            "return": [
                "Any"
            ],
            "arguments": {
                "num_actions": [],
                "state_shape": [],
                "hidden_layers": []
            }
        },
        "DDQNAgent.__init__": {
            "name": "__init__",
            "location": 63,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "num_actions": [],
                "state_shape": [],
                "gamma": [],
                "target_update_freq": [],
                "prebuilt_model": []
            }
        },
        "DDQNAgent.act": {
            "name": "act",
            "location": 82,
            "return": [
                "Any"
            ],
            "arguments": {
                "self": [],
                "state": []
            }
        },
        "DDQNAgent.process_observation": {
            "name": "process_observation",
            "location": 86,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "state": [],
                "action": [],
                "reward": [],
                "next_state": [],
                "done": []
            }
        },
        "DDQNAgent.train": {
            "name": "train",
            "location": 91,
            "return": [
                "Any"
            ],
            "arguments": {
                "self": [],
                "step_num": [],
                "batch_size": [],
                "epochs": []
            }
        },
        "DDQNAgent._update_target_model": {
            "name": "_update_target_model",
            "location": 112,
            "return": [
                "None"
            ],
            "arguments": {
                "self": []
            }
        },
        "DDQNAgent._observations_to_train_data": {
            "name": "_observations_to_train_data",
            "location": 115,
            "return": [
                "Tuple[(_T0, Any)]"
            ],
            "arguments": {
                "self": [],
                "states": [
                    "_T0@@"
                ],
                "actions": [],
                "rewards": [],
                "next_states": [],
                "dones": []
            }
        },
        "DDQNAgent.Q": {
            "name": "Q",
            "location": 129,
            "return": [
                "Any"
            ],
            "arguments": {
                "self": [],
                "states": [],
                "use_target": []
            }
        },
        "DDQNAgent.policy": {
            "name": "policy",
            "location": 142,
            "return": [
                "Any"
            ],
            "arguments": {
                "self": [],
                "states": [],
                "use_target": []
            }
        },
        "DDQNAgent.V": {
            "name": "V",
            "location": 146,
            "return": [
                "Any"
            ],
            "arguments": {
                "self": [],
                "states": [],
                "use_target": []
            }
        },
        "DDQNAgent.save": {
            "name": "save",
            "location": 150,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "file_path": []
            }
        },
        "DDQNAgent.from_h5": {
            "name": "from_h5",
            "location": 158,
            "return": [
                "DDQNAgent"
            ],
            "arguments": {
                "file_path": [],
                "gamma": [],
                "target_update_freq": []
            }
        }
    },
    "my_agents-master/my_agents/agents/distributional_agent.py": {
        "build_distributional_network": {
            "name": "build_distributional_network",
            "location": 19,
            "return": [
                "Any"
            ],
            "arguments": {
                "num_actions": [],
                "state_shape": [],
                "num_atoms": [],
                "hidden_layers": []
            }
        },
        "DistributionalAgent.__init__": {
            "name": "__init__",
            "location": 61,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "num_actions": [],
                "state_shape": [],
                "v_min": [],
                "v_max": [],
                "num_atoms": [],
                "gamma": [],
                "target_update_freq": [],
                "prebuilt_model": []
            }
        },
        "DistributionalAgent.act": {
            "name": "act",
            "location": 83,
            "return": [
                "Any"
            ],
            "arguments": {
                "self": [],
                "state": []
            }
        },
        "DistributionalAgent.process_observation": {
            "name": "process_observation",
            "location": 87,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "state": [],
                "action": [],
                "reward": [],
                "next_state": [],
                "done": []
            }
        },
        "DistributionalAgent.train": {
            "name": "train",
            "location": 92,
            "return": [
                "Any"
            ],
            "arguments": {
                "self": [],
                "step_num": [],
                "batch_size": [],
                "epochs": []
            }
        },
        "DistributionalAgent._update_target_model": {
            "name": "_update_target_model",
            "location": 113,
            "return": [
                "None"
            ],
            "arguments": {
                "self": []
            }
        },
        "DistributionalAgent._observations_to_train_data": {
            "name": "_observations_to_train_data",
            "location": 116,
            "return": [
                "Tuple[(_T0, list)]"
            ],
            "arguments": {
                "self": [],
                "states": [
                    "_T0@@"
                ],
                "actions": [],
                "rewards": [],
                "next_states": [],
                "dones": []
            }
        },
        "DistributionalAgent.Z": {
            "name": "Z",
            "location": 149,
            "return": [
                "np.ndarray"
            ],
            "arguments": {
                "self": [],
                "states": []
            }
        },
        "DistributionalAgent.Q": {
            "name": "Q",
            "location": 156,
            "return": [
                "Any"
            ],
            "arguments": {
                "self": [],
                "states": []
            }
        },
        "DistributionalAgent.policy": {
            "name": "policy",
            "location": 168,
            "return": [
                "Any"
            ],
            "arguments": {
                "self": [],
                "states": []
            }
        },
        "DistributionalAgent.V": {
            "name": "V",
            "location": 172,
            "return": [
                "Any"
            ],
            "arguments": {
                "self": [],
                "states": []
            }
        },
        "DistributionalAgent.save": {
            "name": "save",
            "location": 176,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "file_path": []
            }
        },
        "DistributionalAgent.from_h5": {
            "name": "from_h5",
            "location": 184,
            "return": [
                "DistributionalAgent"
            ],
            "arguments": {
                "file_path": [],
                "v_min": [],
                "v_max": [],
                "gamma": [],
                "target_update_freq": []
            }
        },
        "DistributionalAgent.Distribution.__init__": {
            "name": "__init__",
            "location": 47,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "v_min": [],
                "v_max": [],
                "num_atoms": []
            }
        },
        "DistributionalAgent.Distribution.project_to_distribution": {
            "name": "project_to_distribution",
            "location": 54,
            "return": [
                "Tuple[(Any, Any, Any)]"
            ],
            "arguments": {
                "self": [],
                "values": []
            }
        }
    },
    "my_agents-master/my_agents/agents/dqn_agent.py": {
        "build_dense_network": {
            "name": "build_dense_network",
            "location": 24,
            "return": [
                "Any"
            ],
            "arguments": {
                "num_actions": [],
                "state_shape": [],
                "hidden_layers": []
            }
        },
        "DQNAgent.__init__": {
            "name": "__init__",
            "location": 65,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "num_actions": [],
                "state_shape": [],
                "gamma": [],
                "target_update_freq": [],
                "prebuilt_model": []
            }
        },
        "DQNAgent.act": {
            "name": "act",
            "location": 84,
            "return": [
                "Any"
            ],
            "arguments": {
                "self": [],
                "state": []
            }
        },
        "DQNAgent.process_observation": {
            "name": "process_observation",
            "location": 88,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "state": [],
                "action": [],
                "reward": [],
                "next_state": [],
                "done": []
            }
        },
        "DQNAgent.train": {
            "name": "train",
            "location": 93,
            "return": [
                "Any"
            ],
            "arguments": {
                "self": [],
                "step_num": [],
                "batch_size": [],
                "epochs": []
            }
        },
        "DQNAgent._update_target_model": {
            "name": "_update_target_model",
            "location": 114,
            "return": [
                "None"
            ],
            "arguments": {
                "self": []
            }
        },
        "DQNAgent._observations_to_train_data": {
            "name": "_observations_to_train_data",
            "location": 117,
            "return": [
                "Tuple[(_T0, Any)]"
            ],
            "arguments": {
                "self": [],
                "states": [
                    "_T0@@"
                ],
                "actions": [],
                "rewards": [],
                "next_states": [],
                "dones": []
            }
        },
        "DQNAgent.Q": {
            "name": "Q",
            "location": 131,
            "return": [
                "Any"
            ],
            "arguments": {
                "self": [],
                "states": []
            }
        },
        "DQNAgent.policy": {
            "name": "policy",
            "location": 140,
            "return": [
                "Any"
            ],
            "arguments": {
                "self": [],
                "states": []
            }
        },
        "DQNAgent.V": {
            "name": "V",
            "location": 144,
            "return": [
                "Any"
            ],
            "arguments": {
                "self": [],
                "states": []
            }
        },
        "DQNAgent.save": {
            "name": "save",
            "location": 148,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "file_path": []
            }
        },
        "DQNAgent.from_h5": {
            "name": "from_h5",
            "location": 156,
            "return": [
                "DQNAgent"
            ],
            "arguments": {
                "file_path": [],
                "gamma": [],
                "target_update_freq": []
            }
        }
    },
    "my_agents-master/my_agents/agents/nstep_agent.py": {
        "build_dense_network": {
            "name": "build_dense_network",
            "location": 21,
            "return": [
                "Any"
            ],
            "arguments": {
                "num_actions": [],
                "state_shape": [],
                "hidden_layers": []
            }
        },
        "NStepDDQNAgent.__init__": {
            "name": "__init__",
            "location": 61,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "num_actions": [],
                "state_shape": [],
                "update_horizon": [],
                "gamma": [],
                "target_update_freq": [],
                "prebuilt_model": []
            }
        },
        "NStepDDQNAgent.act": {
            "name": "act",
            "location": 82,
            "return": [
                "Any"
            ],
            "arguments": {
                "self": [],
                "state": []
            }
        },
        "NStepDDQNAgent.process_observation": {
            "name": "process_observation",
            "location": 86,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "state": [],
                "action": [],
                "reward": [],
                "next_state": [],
                "done": []
            }
        },
        "NStepDDQNAgent.train": {
            "name": "train",
            "location": 91,
            "return": [
                "Any"
            ],
            "arguments": {
                "self": [],
                "step_num": [],
                "batch_size": [],
                "epochs": []
            }
        },
        "NStepDDQNAgent._sample_n_transitions": {
            "name": "_sample_n_transitions",
            "location": 113,
            "return": [
                "Tuple[(list, list, list, list, list, list)]"
            ],
            "arguments": {
                "self": [],
                "batch_size": []
            }
        },
        "NStepDDQNAgent._update_target_model": {
            "name": "_update_target_model",
            "location": 141,
            "return": [
                "None"
            ],
            "arguments": {
                "self": []
            }
        },
        "NStepDDQNAgent._observations_to_train_data": {
            "name": "_observations_to_train_data",
            "location": 144,
            "return": [
                "Tuple[(_T0, Any)]"
            ],
            "arguments": {
                "self": [],
                "states": [
                    "_T0@@"
                ],
                "actions": [],
                "rewards": [],
                "next_states": [],
                "dones": [],
                "gammas": []
            }
        },
        "NStepDDQNAgent.Q": {
            "name": "Q",
            "location": 159,
            "return": [
                "Any"
            ],
            "arguments": {
                "self": [],
                "states": [],
                "use_target": []
            }
        },
        "NStepDDQNAgent.policy": {
            "name": "policy",
            "location": 172,
            "return": [
                "Any"
            ],
            "arguments": {
                "self": [],
                "states": [],
                "use_target": []
            }
        },
        "NStepDDQNAgent.V": {
            "name": "V",
            "location": 176,
            "return": [
                "Any"
            ],
            "arguments": {
                "self": [],
                "states": [],
                "use_target": []
            }
        },
        "NStepDDQNAgent.save": {
            "name": "save",
            "location": 180,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "file_path": []
            }
        },
        "NStepDDQNAgent.from_h5": {
            "name": "from_h5",
            "location": 188,
            "return": [
                "NStepDDQNAgent"
            ],
            "arguments": {
                "file_path": [],
                "update_horizon": [],
                "gamma": [],
                "target_update_freq": []
            }
        }
    },
    "my_agents-master/my_agents/agents/prioritized_ddqn_agent.py": {
        "build_dense_network": {
            "name": "build_dense_network",
            "location": 16,
            "return": [
                "Any"
            ],
            "arguments": {
                "num_actions": [],
                "state_shape": [],
                "hidden_layers": []
            }
        },
        "PrioritizedDDQNAgent.__init__": {
            "name": "__init__",
            "location": 55,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "num_actions": [],
                "state_shape": [],
                "gamma": [],
                "target_update_freq": [],
                "prebuilt_model": []
            }
        },
        "PrioritizedDDQNAgent.act": {
            "name": "act",
            "location": 74,
            "return": [
                "Any"
            ],
            "arguments": {
                "self": [],
                "state": []
            }
        },
        "PrioritizedDDQNAgent.process_observation": {
            "name": "process_observation",
            "location": 78,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "state": [],
                "action": [],
                "reward": [],
                "next_state": [],
                "done": []
            }
        },
        "PrioritizedDDQNAgent.train": {
            "name": "train",
            "location": 83,
            "return": [
                "Any"
            ],
            "arguments": {
                "self": [],
                "step_num": [],
                "batch_size": [],
                "epochs": []
            }
        },
        "PrioritizedDDQNAgent._update_target_model": {
            "name": "_update_target_model",
            "location": 108,
            "return": [
                "None"
            ],
            "arguments": {
                "self": []
            }
        },
        "PrioritizedDDQNAgent._observations_to_train_data": {
            "name": "_observations_to_train_data",
            "location": 111,
            "return": [
                "Tuple[(_T0, Any)]"
            ],
            "arguments": {
                "self": [],
                "states": [
                    "_T0@@"
                ],
                "actions": [],
                "rewards": [],
                "next_states": [],
                "dones": []
            }
        },
        "PrioritizedDDQNAgent.Q": {
            "name": "Q",
            "location": 125,
            "return": [
                "Any"
            ],
            "arguments": {
                "self": [],
                "states": [],
                "use_target": []
            }
        },
        "PrioritizedDDQNAgent.policy": {
            "name": "policy",
            "location": 138,
            "return": [
                "Any"
            ],
            "arguments": {
                "self": [],
                "states": [],
                "use_target": []
            }
        },
        "PrioritizedDDQNAgent.V": {
            "name": "V",
            "location": 142,
            "return": [
                "Any"
            ],
            "arguments": {
                "self": [],
                "states": [],
                "use_target": []
            }
        },
        "PrioritizedDDQNAgent.save": {
            "name": "save",
            "location": 146,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "file_path": []
            }
        },
        "PrioritizedDDQNAgent.from_h5": {
            "name": "from_h5",
            "location": 154,
            "return": [
                "PrioritizedDDQNAgent"
            ],
            "arguments": {
                "file_path": [],
                "gamma": [],
                "target_update_freq": []
            }
        }
    },
    "my_agents-master/my_agents/agents/prioritized_memory.py": {
        "SumTree.__init__": {
            "name": "__init__",
            "location": 15,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "capacity": []
            }
        },
        "SumTree.add": {
            "name": "add",
            "location": 22,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "priority": [],
                "data": []
            }
        },
        "SumTree.update": {
            "name": "update",
            "location": 36,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "tree_index": [],
                "priority": []
            }
        },
        "SumTree.get_leaf": {
            "name": "get_leaf",
            "location": 46,
            "return": [
                "Tuple[(int, Any, Any)]"
            ],
            "arguments": {
                "self": [],
                "v": []
            }
        },
        "SumTree.total_priority": {
            "name": "total_priority",
            "location": 73,
            "return": [],
            "arguments": {
                "self": []
            }
        },
        "SumTree.max_priority": {
            "name": "max_priority",
            "location": 77,
            "return": [],
            "arguments": {
                "self": []
            }
        },
        "SumTree.min_priority": {
            "name": "min_priority",
            "location": 81,
            "return": [],
            "arguments": {
                "self": []
            }
        },
        "SumTree.__len__": {
            "name": "__len__",
            "location": 84,
            "return": [
                "int"
            ],
            "arguments": {
                "self": []
            }
        },
        "PrioritizedMemory.__init__": {
            "name": "__init__",
            "location": 95,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "capacity": [],
                "alpha": [],
                "beta": [],
                "max_error": []
            }
        },
        "PrioritizedMemory.store": {
            "name": "store",
            "location": 103,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "experience": []
            }
        },
        "PrioritizedMemory.sample": {
            "name": "sample",
            "location": 110,
            "return": [
                "Tuple[(Any, list, Any)]"
            ],
            "arguments": {
                "self": [],
                "batch_size": []
            }
        },
        "PrioritizedMemory.batch_update": {
            "name": "batch_update",
            "location": 141,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "tree_idx": [],
                "abs_errors": []
            }
        },
        "PrioritizedMemory.__len__": {
            "name": "__len__",
            "location": 150,
            "return": [
                "int"
            ],
            "arguments": {
                "self": []
            }
        }
    },
    "my_agents-master/my_agents/agents/table_agent.py": {
        "TableAgent.__init__": {
            "name": "__init__",
            "location": 6,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "num_actions": [],
                "num_states": [],
                "gamma": [],
                "alpha": []
            }
        },
        "TableAgent.act": {
            "name": "act",
            "location": 14,
            "return": [
                "Any"
            ],
            "arguments": {
                "self": [],
                "state": []
            }
        },
        "TableAgent.process_observation": {
            "name": "process_observation",
            "location": 18,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "state": [],
                "action": [],
                "reward": [],
                "next_state": [],
                "done": []
            }
        },
        "TableAgent.train": {
            "name": "train",
            "location": 26,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "step_num": []
            }
        },
        "TableAgent.Q": {
            "name": "Q",
            "location": 30,
            "return": [
                "Any"
            ],
            "arguments": {
                "self": [],
                "state": []
            }
        },
        "TableAgent.policy": {
            "name": "policy",
            "location": 34,
            "return": [
                "Any"
            ],
            "arguments": {
                "self": [],
                "state": []
            }
        },
        "TableAgent.V": {
            "name": "V",
            "location": 38,
            "return": [
                "Any"
            ],
            "arguments": {
                "self": [],
                "state": []
            }
        },
        "TableAgent.print_q_map": {
            "name": "print_q_map",
            "location": 42,
            "return": [
                "None"
            ],
            "arguments": {
                "self": []
            }
        }
    },
    "my_agents-master/my_agents/core/runner.py": {
        "constant_decay_epsilon": {
            "name": "constant_decay_epsilon",
            "location": 20,
            "return": [
                "Any"
            ],
            "arguments": {
                "epoch": [],
                "initial_epsilon": [],
                "decay_rate": [],
                "min_epsilon": []
            }
        },
        "Runner.__init__": {
            "name": "__init__",
            "location": 30,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "env": [],
                "serializer": [],
                "agent": [],
                "epsilon_policy": [],
                "training_period": [],
                "max_episode_steps": []
            }
        },
        "Runner.warm_up": {
            "name": "warm_up",
            "location": 44,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "num_steps": []
            }
        },
        "Runner.train": {
            "name": "train",
            "location": 56,
            "return": [
                "Any"
            ],
            "arguments": {
                "self": [],
                "num_epochs": [],
                "num_episodes": [],
                "render_frequency": []
            }
        },
        "Runner.demonstrate": {
            "name": "demonstrate",
            "location": 69,
            "return": [
                "Tuple[(float, Any, Any, int)]"
            ],
            "arguments": {
                "self": [],
                "num_episodes": []
            }
        },
        "Runner.render": {
            "name": "render",
            "location": 78,
            "return": [
                "Tuple[(Any, Any, int)]"
            ],
            "arguments": {
                "self": []
            }
        },
        "Runner.history": {
            "name": "history",
            "location": 84,
            "return": [],
            "arguments": {
                "self": []
            }
        },
        "Runner.run_episode": {
            "name": "run_episode",
            "location": 87,
            "return": [
                "Tuple[(Any, Any, int)]"
            ],
            "arguments": {
                "self": [],
                "epsilon": [],
                "training": [],
                "render": []
            }
        },
        "Runner.run_epoch": {
            "name": "run_epoch",
            "location": 119,
            "return": [
                "Tuple[(float, list, List[int], int)]"
            ],
            "arguments": {
                "self": [],
                "epsilon": [],
                "num_episodes": [],
                "training": [],
                "render_frequency": []
            }
        }
    },
    "my_agents-master/my_agents/core/states.py": {
        "one_hot": {
            "name": "one_hot",
            "location": 3,
            "return": [
                "Any"
            ],
            "arguments": {
                "size": [],
                "idx": []
            }
        },
        "StateSerializer.__init__": {
            "name": "__init__",
            "location": 16,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "state_shape": []
            }
        },
        "StateSerializer.serialize": {
            "name": "serialize",
            "location": 20,
            "return": [
                "_T0@@"
            ],
            "arguments": {
                "self": [],
                "state": [
                    "_T0@@"
                ]
            }
        },
        "StateSerializer.deserialize": {
            "name": "deserialize",
            "location": 25,
            "return": [
                "_T0@@"
            ],
            "arguments": {
                "self": [],
                "state": [
                    "_T0@@"
                ]
            }
        },
        "StateSerializer.shape": {
            "name": "shape",
            "location": 31,
            "return": [],
            "arguments": {
                "self": []
            }
        },
        "StateSerializer.from_num_states": {
            "name": "from_num_states",
            "location": 35,
            "return": [
                "StateSerializer"
            ],
            "arguments": {
                "num_states": []
            }
        }
    },
    "my_agents-master/my_agents/core/visualization.py": {
        "rolling_mean": {
            "name": "rolling_mean",
            "location": 12,
            "return": [
                "Any"
            ],
            "arguments": {
                "history": [],
                "window": [],
                "label": [],
                "axis": [],
                "show": []
            }
        }
    }
}
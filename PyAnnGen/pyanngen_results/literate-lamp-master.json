{
    "literate-lamp-master/literate_lamp/args.py": {
        "list_models": {
            "name": "list_models",
            "location": 12,
            "return": [
                "None"
            ],
            "arguments": {
                "models": [
                    "List[str]"
                ]
            }
        },
        "get_args": {
            "name": "get_args",
            "location": 19,
            "return": [
                "DotDict",
                "str",
                "Iterable[str]",
                "List[str]"
            ],
            "arguments": {
                "arguments": [
                    "Optional[List[str]]"
                ]
            }
        },
        "get_args.preprocessed_name": {
            "name": "preprocessed_name",
            "location": 138,
            "return": [
                "str",
                "Optional[str]"
            ],
            "arguments": {
                "split_type": [
                    "str",
                    "Optional[str]",
                    "Dict[str, Any]"
                ]
            }
        }
    },
    "literate-lamp-master/literate_lamp/common.py": {
        "set_args": {
            "name": "set_args",
            "location": 30,
            "return": [
                "None"
            ],
            "arguments": {
                "args": []
            }
        },
        "get_seq2seq": {
            "name": "get_seq2seq",
            "location": 35,
            "return": [
                "Callable[([int, int, int, bool, float], Any)]"
            ],
            "arguments": {
                "encoder_type": [
                    "Optional[str]"
                ]
            }
        },
        "get_encoder": {
            "name": "get_encoder",
            "location": 48,
            "return": [
                "Callable[([int, int, int, bool, float], Any)]"
            ],
            "arguments": {
                "encoder_type": [
                    "Optional[str]"
                ]
            }
        },
        "get_word_embeddings": {
            "name": "get_word_embeddings",
            "location": 64,
            "return": [],
            "arguments": {
                "vocabulary": []
            }
        },
        "build_dmn": {
            "name": "build_dmn",
            "location": 81,
            "return": [
                "Dmn"
            ],
            "arguments": {
                "vocabulary": []
            }
        },
        "build_relational_xl": {
            "name": "build_relational_xl",
            "location": 115,
            "return": [
                "RelationalXL"
            ],
            "arguments": {
                "vocabulary": []
            }
        },
        "build_advanced_xlnet": {
            "name": "build_advanced_xlnet",
            "location": 167,
            "return": [
                "AdvancedXLNetClassifier"
            ],
            "arguments": {
                "vocab": []
            }
        },
        "build_simple_xlnet": {
            "name": "build_simple_xlnet",
            "location": 210,
            "return": [
                "SimpleXLNetClassifier"
            ],
            "arguments": {
                "vocabulary": []
            }
        },
        "build_dcmn": {
            "name": "build_dcmn",
            "location": 219,
            "return": [
                "Dcmn"
            ],
            "arguments": {
                "vocabulary": []
            }
        },
        "build_rel_han": {
            "name": "build_rel_han",
            "location": 242,
            "return": [
                "RelationalHan"
            ],
            "arguments": {
                "vocab": []
            }
        },
        "build_relational_transformer": {
            "name": "build_relational_transformer",
            "location": 341,
            "return": [
                "RelationalTransformerModel"
            ],
            "arguments": {
                "vocab": []
            }
        },
        "build_hierarchical_attn_net": {
            "name": "build_hierarchical_attn_net",
            "location": 427,
            "return": [
                "HierarchicalAttentionNetwork"
            ],
            "arguments": {
                "vocab": []
            }
        },
        "build_advanced_attn_bert": {
            "name": "build_advanced_attn_bert",
            "location": 499,
            "return": [
                "AdvancedAttentionBertClassifier"
            ],
            "arguments": {
                "vocab": []
            }
        },
        "build_hierarchical_bert": {
            "name": "build_hierarchical_bert",
            "location": 554,
            "return": [
                "HierarchicalBert"
            ],
            "arguments": {
                "vocab": []
            }
        },
        "build_zero_trian": {
            "name": "build_zero_trian",
            "location": 605,
            "return": [
                "ZeroTrian"
            ],
            "arguments": {
                "vocab": []
            }
        },
        "build_simple_trian": {
            "name": "build_simple_trian",
            "location": 694,
            "return": [
                "SimpleTrian"
            ],
            "arguments": {
                "vocab": []
            }
        },
        "build_advanced_bert": {
            "name": "build_advanced_bert",
            "location": 785,
            "return": [
                "AdvancedBertClassifier"
            ],
            "arguments": {
                "vocab": []
            }
        },
        "build_simple_bert": {
            "name": "build_simple_bert",
            "location": 830,
            "return": [
                "SimpleBertClassifier"
            ],
            "arguments": {
                "vocab": []
            }
        },
        "build_baseline": {
            "name": "build_baseline",
            "location": 846,
            "return": [
                "BaselineClassifier"
            ],
            "arguments": {
                "vocab": []
            }
        },
        "build_attentive_reader": {
            "name": "build_attentive_reader",
            "location": 869,
            "return": [
                "AttentiveReader"
            ],
            "arguments": {
                "vocab": []
            }
        },
        "build_trian": {
            "name": "build_trian",
            "location": 898,
            "return": [
                "Trian"
            ],
            "arguments": {
                "vocab": []
            }
        },
        "create_reader": {
            "name": "create_reader",
            "location": 998,
            "return": [
                "SimpleMcScriptReader",
                "FullTrianReader",
                "SimpleBertReader",
                "SimpleTrianReader",
                "RelationBertReader",
                "SimpleXLNetReader",
                "RelationXLNetReader",
                "ExtendedXLNetReader",
                "SentenceReader"
            ],
            "arguments": {
                "reader_type": [
                    "str"
                ]
            }
        },
        "get_modelfn_reader": {
            "name": "get_modelfn_reader",
            "location": 1063,
            "return": [
                "Tuple[(Callable[([Any], Any)], str)]"
            ],
            "arguments": {}
        },
        "split_list": {
            "name": "split_list",
            "location": 1071,
            "return": [
                "Dict[(str, list)]"
            ],
            "arguments": {
                "data": [
                    "list"
                ]
            }
        },
        "evaluate": {
            "name": "evaluate",
            "location": 1081,
            "return": [
                "None"
            ],
            "arguments": {
                "model": [],
                "reader": [],
                "test_data": [
                    "list"
                ]
            }
        },
        "print_dmn_instance": {
            "name": "print_dmn_instance",
            "location": 1111,
            "return": [
                "None"
            ],
            "arguments": {
                "instance": [],
                "prediction": []
            }
        },
        "print_base_instance": {
            "name": "print_base_instance",
            "location": 1124,
            "return": [
                "None"
            ],
            "arguments": {
                "instance": [],
                "prediction": []
            }
        },
        "process_dmn_list": {
            "name": "process_dmn_list",
            "location": 1137,
            "return": [
                "str"
            ],
            "arguments": {
                "fields": []
            }
        },
        "print_xlnet_instance": {
            "name": "print_xlnet_instance",
            "location": 1143,
            "return": [
                "None"
            ],
            "arguments": {
                "instance": [],
                "probability": []
            }
        },
        "process_bert_list": {
            "name": "process_bert_list",
            "location": 1168,
            "return": [
                "Tuple[(str, str, str)]"
            ],
            "arguments": {
                "fields": []
            }
        },
        "print_bert_instance": {
            "name": "print_bert_instance",
            "location": 1181,
            "return": [
                "None"
            ],
            "arguments": {
                "instance": [],
                "prediction": []
            }
        },
        "error_analysis": {
            "name": "error_analysis",
            "location": 1195,
            "return": [
                "None"
            ],
            "arguments": {
                "model": [],
                "test_data": [
                    "list"
                ],
                "sample_size": [
                    "int"
                ]
            }
        },
        "print_instance": {
            "name": "print_instance",
            "location": 1231,
            "return": [
                "None"
            ],
            "arguments": {
                "passage_id": [
                    "str"
                ],
                "question_id": [
                    "str"
                ],
                "question_type": [
                    "str"
                ],
                "passage": [
                    "str"
                ],
                "question": [
                    "str"
                ],
                "answer1": [
                    "str"
                ],
                "answer2": [
                    "str"
                ],
                "probability": [],
                "label": [
                    "int"
                ]
            }
        },
        "print_xlnet_instance.clean": {
            "name": "clean",
            "location": 1146,
            "return": [],
            "arguments": {
                "string": []
            }
        }
    },
    "literate-lamp-master/literate_lamp/conceptnet.py": {
        "triple_as_sentence": {
            "name": "triple_as_sentence",
            "location": 95,
            "return": [
                "str"
            ],
            "arguments": {
                "triple": [
                    "Tuple[(str, str, str)]"
                ]
            }
        },
        "ConceptNet.__init__": {
            "name": "__init__",
            "location": 19,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "conceptnet_path": [
                    "Optional[pathlib.Path]"
                ]
            }
        },
        "ConceptNet.get_relation": {
            "name": "get_relation",
            "location": 35,
            "return": [
                "str"
            ],
            "arguments": {
                "self": [],
                "word1": [
                    "str"
                ],
                "word2": [
                    "str"
                ]
            }
        },
        "ConceptNet.get_all_text_query_triples": {
            "name": "get_all_text_query_triples",
            "location": 50,
            "return": [
                "Set[Tuple[(str, str, str)]]"
            ],
            "arguments": {
                "self": [],
                "text": [
                    "Sequence[str]"
                ],
                "query": [
                    "Sequence[str]"
                ]
            }
        },
        "ConceptNet.get_text_query_relations": {
            "name": "get_text_query_relations",
            "location": 73,
            "return": [
                "List[str]"
            ],
            "arguments": {
                "self": [],
                "text": [
                    "Sequence[str]"
                ],
                "query": [
                    "Sequence[str]"
                ]
            }
        }
    },
    "literate-lamp-master/literate_lamp/graph.py": {
        "main": {
            "name": "main",
            "location": 5,
            "return": [
                "None"
            ],
            "arguments": {}
        }
    },
    "literate-lamp-master/literate_lamp/layers.py": {
        "learned_embeddings": {
            "name": "learned_embeddings",
            "location": 243,
            "return": [
                "BasicTextFieldEmbedder",
                "int",
                "Callable",
                "str"
            ],
            "arguments": {
                "vocab": [
                    "str",
                    "torch.Tensor",
                    "int"
                ],
                "dimension": [
                    "int"
                ],
                "namespace": [
                    "str"
                ]
            }
        },
        "bert_embeddings": {
            "name": "bert_embeddings",
            "location": 253,
            "return": [
                "BasicTextFieldEmbedder",
                "int",
                "Callable",
                "str"
            ],
            "arguments": {
                "pretrained_model": [
                    "pathlib.Path"
                ],
                "training": [
                    "bool"
                ],
                "top_layer_only": [
                    "bool"
                ]
            }
        },
        "xlnet_embeddings": {
            "name": "xlnet_embeddings",
            "location": 269,
            "return": [
                "BasicTextFieldEmbedder",
                "int",
                "Callable",
                "str"
            ],
            "arguments": {
                "config_path": [
                    "pathlib.Path"
                ],
                "model_path": [
                    "pathlib.Path"
                ],
                "window_size": [
                    "Optional[int]"
                ],
                "training": [
                    "bool"
                ],
                "top_layer_only": [
                    "bool"
                ]
            }
        },
        "glove_embeddings": {
            "name": "glove_embeddings",
            "location": 289,
            "return": [
                "BasicTextFieldEmbedder",
                "int",
                "Callable",
                "str"
            ],
            "arguments": {
                "vocab": [
                    "str",
                    "Union[pathlib.Path, str]",
                    "pathlib.Path",
                    "pathlib2.Path"
                ],
                "file_path": [
                    "pathlib.Path"
                ],
                "dimension": [
                    "int"
                ],
                "training": [
                    "bool"
                ],
                "namespace": [
                    "str"
                ]
            }
        },
        "lstm_seq2seq": {
            "name": "lstm_seq2seq",
            "location": 303,
            "return": [
                "PytorchSeq2SeqWrapper",
                "int",
                "Callable",
                "str"
            ],
            "arguments": {
                "input_dim": [
                    "int"
                ],
                "output_dim": [
                    "int"
                ],
                "num_layers": [
                    "int"
                ],
                "bidirectional": [
                    "bool"
                ],
                "dropout": [
                    "float"
                ]
            }
        },
        "gru_seq2seq": {
            "name": "gru_seq2seq",
            "location": 315,
            "return": [
                "PytorchSeq2SeqWrapper",
                "int",
                "Callable",
                "str"
            ],
            "arguments": {
                "input_dim": [
                    "int"
                ],
                "output_dim": [
                    "int"
                ],
                "num_layers": [
                    "int"
                ],
                "bidirectional": [
                    "bool"
                ],
                "dropout": [
                    "float"
                ]
            }
        },
        "transformer_seq2seq": {
            "name": "transformer_seq2seq",
            "location": 327,
            "return": [
                "str",
                "TransformerEncoder",
                "StackedSelfAttentionEncoder"
            ],
            "arguments": {
                "input_dim": [
                    "int"
                ],
                "model_dim": [
                    "int"
                ],
                "feedforward_hidden_dim": [
                    "int"
                ],
                "num_layers": [
                    "int"
                ],
                "projection_dim": [
                    "int"
                ],
                "num_attention_heads": [
                    "int"
                ],
                "ttype": [
                    "str"
                ],
                "dropout": [
                    "float"
                ]
            }
        },
        "lstm_encoder": {
            "name": "lstm_encoder",
            "location": 358,
            "return": [
                "PytorchSeq2VecWrapper",
                "int",
                "Callable",
                "str"
            ],
            "arguments": {
                "input_dim": [
                    "int"
                ],
                "output_dim": [
                    "int"
                ],
                "num_layers": [
                    "int"
                ],
                "bidirectional": [
                    "bool"
                ],
                "dropout": [
                    "float"
                ]
            }
        },
        "gru_encoder": {
            "name": "gru_encoder",
            "location": 370,
            "return": [
                "PytorchSeq2VecWrapper",
                "int",
                "Callable",
                "str"
            ],
            "arguments": {
                "input_dim": [
                    "int"
                ],
                "output_dim": [
                    "int"
                ],
                "num_layers": [
                    "int"
                ],
                "bidirectional": [
                    "bool"
                ],
                "dropout": [
                    "float"
                ]
            }
        },
        "cnn_encoder": {
            "name": "cnn_encoder",
            "location": 382,
            "return": [
                "CnnEncoder",
                "str",
                "dict"
            ],
            "arguments": {
                "input_dim": [
                    "int"
                ],
                "output_dim": [
                    "int"
                ],
                "num_filters": [
                    "int"
                ],
                "ngram_filter_sizes": [
                    "Tuple[(int, ...)]"
                ]
            }
        },
        "BilinearMatrixAttention.__init__": {
            "name": "__init__",
            "location": 52,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "matrix1_dim": [
                    "int"
                ],
                "matrix2_dim": [
                    "int"
                ],
                "normalise": [
                    "bool"
                ]
            }
        },
        "BilinearMatrixAttention._forward_internal": {
            "name": "_forward_internal",
            "location": 61,
            "return": [
                "int",
                "Callable",
                "str"
            ],
            "arguments": {
                "self": [],
                "matrix1": [
                    "int",
                    "str",
                    "Callable",
                    "typing.Callable[, ]"
                ],
                "matrix2": [
                    "int",
                    "str",
                    "Callable",
                    "typing.Callable[, ]"
                ]
            }
        },
        "LinearAttention.__init__": {
            "name": "__init__",
            "location": 78,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "input_dim": [
                    "int"
                ],
                "bias": [
                    "bool"
                ]
            }
        },
        "LinearAttention.forward": {
            "name": "forward",
            "location": 85,
            "return": [
                "str",
                "int",
                "Optional[str]"
            ],
            "arguments": {
                "self": [],
                "inputs": [
                    "float",
                    "busboy.util.Maybe[psycopg2.extensions.connection]",
                    "str",
                    "Optional[str]",
                    "None"
                ],
                "mask": [
                    "None",
                    "Optional[str]",
                    "int",
                    "Optional[int]",
                    "float",
                    "Optional[float]",
                    "bool"
                ]
            }
        },
        "LinearAttention.get_input_dim": {
            "name": "get_input_dim",
            "location": 116,
            "return": [
                "int"
            ],
            "arguments": {
                "self": []
            }
        },
        "LinearAttention.get_output_dim": {
            "name": "get_output_dim",
            "location": 120,
            "return": [
                "int"
            ],
            "arguments": {
                "self": []
            }
        },
        "LinearSelfAttention.__init__": {
            "name": "__init__",
            "location": 147,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "input_dim": [
                    "int"
                ],
                "normalise": [
                    "bool"
                ],
                "bias": [
                    "bool"
                ]
            }
        },
        "LinearSelfAttention._forward_internal": {
            "name": "_forward_internal",
            "location": 157,
            "return": [
                "int",
                "Callable",
                "tuple[typing.Union[typing.Literal,typing.Literal,typing.Literal,typing.Literal,typing.Literal,typing.Literal,typing.Literal,typing.Literal,typing.Literal,typing.Literal,typing.Literal,typing.Literal,typing.Literal]]",
                "str",
                "typing.Type"
            ],
            "arguments": {
                "self": [],
                "vector": [
                    "int",
                    "str",
                    "Callable",
                    "typing.Callable[, ]"
                ],
                "_": [
                    "int",
                    "str",
                    "typing.Iterable[C]",
                    "Iterable['Context']",
                    "dict",
                    "bool"
                ]
            }
        },
        "BilinearAttention.__init__": {
            "name": "__init__",
            "location": 186,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "vector_dim": [
                    "int"
                ],
                "matrix_dim": [
                    "int"
                ],
                "normalise": [
                    "bool"
                ]
            }
        },
        "BilinearAttention._forward_internal": {
            "name": "_forward_internal",
            "location": 195,
            "return": [
                "int",
                "Callable",
                "str",
                "dict[, ]",
                "list[int]",
                "list[list[int]]"
            ],
            "arguments": {
                "self": [],
                "vector": [
                    "int",
                    "str",
                    "Callable",
                    "typing.Callable[, ]"
                ],
                "matrix": [
                    "int",
                    "str",
                    "Callable",
                    "typing.Callable[, ]"
                ]
            }
        },
        "SequenceAttention.__init__": {
            "name": "__init__",
            "location": 225,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "input_dim": [
                    "int"
                ],
                "activation": [
                    "bool",
                    "str",
                    "None"
                ],
                "normalise": [
                    "bool"
                ]
            }
        },
        "SequenceAttention._forward_internal": {
            "name": "_forward_internal",
            "location": 235,
            "return": [
                "int",
                "Callable",
                "str"
            ],
            "arguments": {
                "self": [],
                "u": [
                    "int"
                ],
                "v": [
                    "int"
                ]
            }
        },
        "MultiHeadAttention.__init__": {
            "name": "__init__",
            "location": 430,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "num_heads": [
                    "int",
                    "float",
                    "str"
                ],
                "query_input_dim": [
                    "dict[, ]",
                    "Optional[dict]",
                    "None",
                    "bool",
                    "allennlp.nn.decoding.decoder_state.DecoderState",
                    "list[C]",
                    "List['cirq.Circuit']",
                    "Optional[\"GraphLogger\"]",
                    "int"
                ],
                "key_input_dim": [
                    "List[str]",
                    "list[str]",
                    "str",
                    "Optional[str]",
                    "None",
                    "Dict[str, List[str]]",
                    "dict[str, list[str]]"
                ],
                "value_input_dim": [
                    "int",
                    "float",
                    "Optional[float]",
                    "None"
                ],
                "attention_dim": [
                    "int",
                    "float",
                    "Optional[Union[int, float]]",
                    "Union[int, float]"
                ],
                "values_dim": [
                    "float",
                    "int",
                    "Sequence[int]"
                ],
                "output_projection_dim": [
                    "int",
                    "str",
                    "float",
                    "None",
                    "bool",
                    "Optional[float]",
                    "Optional[str]"
                ],
                "attention_dropout_prob": [
                    "float",
                    "Optional[dict]",
                    "allennlp.nn.decoding.decoder_state.DecoderState",
                    "List['cirq.Circuit']",
                    "bool",
                    "Optional[\"GraphLogger\"]",
                    "int"
                ]
            }
        },
        "MultiHeadAttention.get_input_dim": {
            "name": "get_input_dim",
            "location": 471,
            "return": [
                "int"
            ],
            "arguments": {
                "self": []
            }
        },
        "MultiHeadAttention.get_output_dim": {
            "name": "get_output_dim",
            "location": 474,
            "return": [
                "int"
            ],
            "arguments": {
                "self": []
            }
        },
        "MultiHeadAttention.is_bidirectional": {
            "name": "is_bidirectional",
            "location": 478,
            "return": [
                "bool"
            ],
            "arguments": {
                "self": []
            }
        },
        "MultiHeadAttention.forward": {
            "name": "forward",
            "location": 482,
            "return": [
                "str",
                "int",
                "Optional[str]"
            ],
            "arguments": {
                "self": [],
                "keys": [
                    "Sequence[Any]",
                    "float",
                    "typing.Sequence[typing.Any]",
                    "str",
                    "list",
                    "list[]"
                ],
                "queries": [
                    "int",
                    "str",
                    "float",
                    "torch.Tensor",
                    "Union[int, float]",
                    "dict[str, torch.LongTensor]",
                    "Dict[str, torch.LongTensor]"
                ],
                "values": [
                    "bytes",
                    "torch.Tensor",
                    "int",
                    "dict[str, str]",
                    "Dict[str, str]"
                ],
                "mask": [
                    "None",
                    "torch.Tensor",
                    "str",
                    "bytes",
                    "Optional[gluonts.model.common.Tensor]",
                    "typing.Sequence[typing.Any]",
                    "Optional[str]",
                    "list[set[int]]",
                    "Optional[Sequence[Any]]",
                    "List[Set[int]]"
                ]
            }
        },
        "HeterogenousSequenceAttention.__init__": {
            "name": "__init__",
            "location": 597,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "u_input_dim": [
                    "int"
                ],
                "v_input_dim": [
                    "int"
                ],
                "projection_dim": [
                    "int"
                ],
                "activation": [
                    "str",
                    "None",
                    "bool",
                    "Tuple[float, float]",
                    "Union[torch.Tensor, float]",
                    "tuple[typing.Union[float,float]]",
                    "float"
                ]
            }
        },
        "HeterogenousSequenceAttention.get_input_dim": {
            "name": "get_input_dim",
            "location": 612,
            "return": [
                "int"
            ],
            "arguments": {
                "self": []
            }
        },
        "HeterogenousSequenceAttention.get_output_dim": {
            "name": "get_output_dim",
            "location": 615,
            "return": [
                "int"
            ],
            "arguments": {
                "self": []
            }
        },
        "HeterogenousSequenceAttention.is_bidirectional": {
            "name": "is_bidirectional",
            "location": 619,
            "return": [
                "bool"
            ],
            "arguments": {
                "self": []
            }
        },
        "HeterogenousSequenceAttention.forward": {
            "name": "forward",
            "location": 623,
            "return": [
                "deoplete.util.UserContext",
                "bool",
                "Optional[Union[str, Any]]"
            ],
            "arguments": {
                "self": [],
                "u": [
                    "torch.Tensor"
                ],
                "v": [
                    "torch.Tensor"
                ],
                "v_mask": [
                    "object",
                    "tuple[typing.Union[int,int]]",
                    "Tuple[int, int]",
                    "str",
                    "Optional[str]",
                    "None"
                ]
            }
        },
        "MultiHeadAttentionV2.__init__": {
            "name": "__init__",
            "location": 668,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "num_heads": [
                    "int",
                    "str"
                ],
                "u_input_dim": [
                    "dict[str, int]",
                    "bool",
                    "Dict[str, int]",
                    "str",
                    "allennlp.data.fields.sequence_field.SequenceField",
                    "int"
                ],
                "v_input_dim": [
                    "int",
                    "float",
                    "str",
                    "bool",
                    "Union[str, float]"
                ],
                "attention_dim": [
                    "int",
                    "float",
                    "Optional[int]",
                    "Union[int, float]",
                    "Optional[float]"
                ],
                "output_projection_dim": [
                    "int",
                    "float",
                    "Dict[int, int]",
                    "dict[int, int]"
                ],
                "attention_dropout_prob": [
                    "float",
                    "int",
                    "bool",
                    "str",
                    "Union[str, float]"
                ]
            }
        },
        "MultiHeadAttentionV2.get_input_dim": {
            "name": "get_input_dim",
            "location": 696,
            "return": [
                "int"
            ],
            "arguments": {
                "self": []
            }
        },
        "MultiHeadAttentionV2.get_output_dim": {
            "name": "get_output_dim",
            "location": 699,
            "return": [
                "int"
            ],
            "arguments": {
                "self": []
            }
        },
        "MultiHeadAttentionV2.is_bidirectional": {
            "name": "is_bidirectional",
            "location": 703,
            "return": [
                "bool"
            ],
            "arguments": {
                "self": []
            }
        },
        "MultiHeadAttentionV2._reshape_outputs": {
            "name": "_reshape_outputs",
            "location": 706,
            "return": [
                "str",
                "Optional[str]",
                "IO[str]",
                "List[int]"
            ],
            "arguments": {
                "self": [],
                "outputs": [
                    "bytes",
                    "str",
                    "torch.FloatTensor"
                ]
            }
        },
        "MultiHeadAttentionV2._reshape_heads": {
            "name": "_reshape_heads",
            "location": 724,
            "return": [
                "str"
            ],
            "arguments": {
                "self": [],
                "x": [
                    "torch.Tensor",
                    "torch.tensor"
                ]
            }
        },
        "MultiHeadAttentionV2._multiply_and_mask": {
            "name": "_multiply_and_mask",
            "location": 737,
            "return": [
                "int",
                "Callable",
                "str"
            ],
            "arguments": {
                "self": [],
                "q": [
                    "str",
                    "int",
                    "typing.Callable[T, None]",
                    "Callable[[T], None]",
                    "dict[str, typing.Any]",
                    "Dict[str, Any]",
                    "bool"
                ],
                "k": [
                    "int",
                    "gluonts.model.common.Tensor",
                    "torch.Tensor",
                    "bool",
                    "Optional[int]",
                    "float"
                ],
                "k_mask": [
                    "int",
                    "torch.Tensor",
                    "gluonts.model.common.Tensor",
                    "bool"
                ]
            }
        },
        "MultiHeadAttentionV2.forward": {
            "name": "forward",
            "location": 754,
            "return": [
                "tuple[]",
                "list",
                "str"
            ],
            "arguments": {
                "self": [],
                "u": [
                    "int",
                    "Union[float, List[float]]",
                    "float",
                    "gluonts.model.common.Tensor"
                ],
                "v": [
                    "Optional[int]",
                    "int",
                    "bool"
                ],
                "u_mask": [
                    "None",
                    "torch.LongTensor",
                    "int",
                    "Optional[int]",
                    "dict[, ]",
                    "str",
                    "Union[int, None]",
                    "typing.Callable[, ]",
                    "Optional[Dict]",
                    "Optional[str]",
                    "Optional[Callable]",
                    "Optional[gluonts.model.common.Tensor]"
                ],
                "v_mask": [
                    "Optional[bool]",
                    "None",
                    "Optional[Dict[str, str]]",
                    "bool",
                    "Optional[int]",
                    "dict[str, str]",
                    "Optional[Dict]",
                    "dict[, ]"
                ]
            }
        },
        "TransformerEncoderBlock.__init__": {
            "name": "__init__",
            "location": 826,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "model_dim": [
                    "int"
                ],
                "attention_dim": [
                    "int"
                ],
                "num_heads": [
                    "int"
                ],
                "feedforward_dim": [
                    "int"
                ],
                "dropout": [
                    "float"
                ]
            }
        },
        "TransformerEncoderBlock.forward": {
            "name": "forward",
            "location": 856,
            "return": [
                "str",
                "list[]",
                "Optional[Union[str, Any]]",
                "int",
                "Set[str]",
                "list[int]",
                "dict"
            ],
            "arguments": {
                "self": [],
                "src": [
                    "str",
                    "Optional[str]",
                    "torch.Tensor",
                    "Optional[Dict]"
                ],
                "src_mask": [
                    "str",
                    "dict[, ]",
                    "int",
                    "torch.Tensor",
                    "pathlib.Path",
                    "dict"
                ]
            }
        },
        "RelationTransformerEncoderBlock.__init__": {
            "name": "__init__",
            "location": 871,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "model_dim": [
                    "int"
                ],
                "attention_dim": [
                    "int"
                ],
                "num_heads": [
                    "int"
                ],
                "feedforward_dim": [
                    "int"
                ],
                "dropout": [
                    "float"
                ]
            }
        },
        "RelationTransformerEncoderBlock._second_stage": {
            "name": "_second_stage",
            "location": 899,
            "return": [
                "str",
                "list[]",
                "Optional[Union[str, Any]]",
                "int",
                "Set[str]",
                "list[int]",
                "dict"
            ],
            "arguments": {
                "self": [],
                "x": [
                    "float",
                    "util.ArrayLike",
                    "gluonts.model.common.Tensor"
                ],
                "attn": [
                    "int",
                    "float"
                ]
            }
        },
        "RelationTransformerEncoderBlock.forward": {
            "name": "forward",
            "location": 909,
            "return": [
                "Tuple[(Any, Any)]"
            ],
            "arguments": {
                "self": [],
                "src": [
                    "float",
                    "int",
                    "torch.Tensor",
                    "pathlib.Path"
                ],
                "aux": [
                    "float",
                    "typing.IO",
                    "str",
                    "int",
                    "torch.Tensor",
                    "pathlib.Path",
                    "IO[bytes]"
                ],
                "src_mask": [
                    "float",
                    "str",
                    "torch.Tensor",
                    "pathlib.Path"
                ],
                "aux_mask": [
                    "float",
                    "str",
                    "torch.Tensor",
                    "pathlib.Path"
                ]
            }
        },
        "TransformerEncoder.__init__": {
            "name": "__init__",
            "location": 958,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "input_dim": [
                    "int",
                    "float",
                    "bool"
                ],
                "model_dim": [
                    "Optional[List[str]]",
                    "str",
                    "list[str]",
                    "None",
                    "Dict[str, int]",
                    "dict[str, int]"
                ],
                "feedforward_hidden_dim": [
                    "int",
                    "float",
                    "Callable",
                    "typing.Callable[, ]"
                ],
                "num_layers": [
                    "int",
                    "float",
                    "str",
                    "bool",
                    "Union[float, int]"
                ],
                "num_attention_heads": [
                    "int",
                    "float",
                    "Callable",
                    "typing.Callable[, ]"
                ],
                "dropout_prob": [
                    "float",
                    "int",
                    "Callable"
                ]
            }
        },
        "TransformerEncoder._reset_parameters": {
            "name": "_reset_parameters",
            "location": 984,
            "return": [
                "None"
            ],
            "arguments": {
                "self": []
            }
        },
        "TransformerEncoder.get_input_dim": {
            "name": "get_input_dim",
            "location": 990,
            "return": [
                "int"
            ],
            "arguments": {
                "self": []
            }
        },
        "TransformerEncoder.get_output_dim": {
            "name": "get_output_dim",
            "location": 994,
            "return": [
                "int"
            ],
            "arguments": {
                "self": []
            }
        },
        "TransformerEncoder.is_bidirectional": {
            "name": "is_bidirectional",
            "location": 998,
            "return": [
                "bool"
            ],
            "arguments": {
                "self": []
            }
        },
        "TransformerEncoder.forward": {
            "name": "forward",
            "location": 1002,
            "return": [
                "str",
                "int",
                "list[pathlib.Path]",
                "Optional[str]",
                "dict[, ]"
            ],
            "arguments": {
                "self": [],
                "inputs": [
                    "int"
                ],
                "mask": [
                    "int",
                    "None",
                    "float",
                    "Optional[Callable[[str], bool]]",
                    "typing.Callable[str, bool]"
                ]
            }
        },
        "RelationalTransformerEncoder.__init__": {
            "name": "__init__",
            "location": 1057,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "src_input_dim": [
                    "bytes",
                    "int",
                    "dict",
                    "dict[, ]",
                    "Dict[str, Any]",
                    "dict[str, typing.Any]",
                    "tuple",
                    "tuple[]"
                ],
                "kb_input_dim": [
                    "int",
                    "typing.Iterator",
                    "Iterator[int]",
                    "str",
                    "float",
                    "None",
                    "apps.common.coininfo.CoinInfo",
                    "Optional[bool]",
                    "tuple[int]",
                    "Tuple[int]"
                ],
                "model_dim": [
                    "str",
                    "bool",
                    "bytes",
                    "typing.IO",
                    "int",
                    "IO[str]"
                ],
                "feedforward_hidden_dim": [
                    "int",
                    "float",
                    "allennlp.data.Vocabulary"
                ],
                "num_layers": [
                    "int",
                    "str",
                    "float",
                    "torch.Tensor"
                ],
                "num_attention_heads": [
                    "int",
                    "float",
                    "allennlp.data.Vocabulary"
                ],
                "dropout_prob": [
                    "float",
                    "int",
                    "allennlp.data.Vocabulary"
                ],
                "return_kb": [
                    "bool",
                    "str",
                    "Sequence[str]",
                    "int"
                ]
            }
        },
        "RelationalTransformerEncoder._reset_parameters": {
            "name": "_reset_parameters",
            "location": 1099,
            "return": [
                "None"
            ],
            "arguments": {
                "self": []
            }
        },
        "RelationalTransformerEncoder.get_input_dim": {
            "name": "get_input_dim",
            "location": 1105,
            "return": [
                "int"
            ],
            "arguments": {
                "self": []
            }
        },
        "RelationalTransformerEncoder.get_output_dim": {
            "name": "get_output_dim",
            "location": 1109,
            "return": [
                "int"
            ],
            "arguments": {
                "self": []
            }
        },
        "RelationalTransformerEncoder.is_bidirectional": {
            "name": "is_bidirectional",
            "location": 1113,
            "return": [
                "bool"
            ],
            "arguments": {
                "self": []
            }
        },
        "RelationalTransformerEncoder.forward": {
            "name": "forward",
            "location": 1117,
            "return": [
                "tuple[typing.Union[dict[str, typing.Any],str,dict[int, str],int,tuple[]]]",
                "str",
                "dict[str, typing.Any]",
                "bool",
                "Mapping[str, Any]"
            ],
            "arguments": {
                "self": [],
                "src": [
                    "str",
                    "Optional[str]",
                    "None"
                ],
                "kb": [
                    "float",
                    "int",
                    "torch.Tensor"
                ],
                "src_mask": [
                    "None",
                    "Optional[torch.Tensor]",
                    "int",
                    "str",
                    "markdown.util.etree.Element"
                ],
                "kb_mask": [
                    "int",
                    "None",
                    "Optional[torch.Tensor]",
                    "bool"
                ]
            }
        }
    },
    "literate-lamp-master/literate_lamp/play.py": {
        "main": {
            "name": "main",
            "location": 15,
            "return": [
                "None"
            ],
            "arguments": {}
        }
    },
    "literate-lamp-master/literate_lamp/predictor.py": {
        "score_questions": {
            "name": "score_questions",
            "location": 58,
            "return": [
                "float"
            ],
            "arguments": {
                "model": [
                    "str",
                    "allennlp.models.Model",
                    "lms.lmsdb.models.User"
                ],
                "output_file": [
                    "TextIO"
                ],
                "testset": [
                    "Sequence"
                ]
            }
        },
        "McScriptPredictor.predict": {
            "name": "predict",
            "location": 19,
            "return": [
                "dict",
                "bool",
                "int",
                "Iterable[str]"
            ],
            "arguments": {
                "self": [],
                "passage_id": [
                    "str",
                    "bool",
                    "Optional[str]",
                    "None"
                ],
                "question_id": [
                    "str",
                    "bool",
                    "Optional[str]",
                    "None"
                ],
                "question_type": [
                    "str",
                    "bool",
                    "Optional[str]",
                    "None"
                ],
                "passage": [
                    "str",
                    "bool",
                    "Optional[str]",
                    "None"
                ],
                "question": [
                    "str",
                    "bool",
                    "Optional[str]",
                    "None"
                ],
                "answer0": [
                    "str",
                    "bool",
                    "Optional[str]",
                    "None"
                ],
                "answer1": [
                    "str",
                    "bool",
                    "Optional[str]",
                    "None"
                ]
            }
        },
        "McScriptPredictor._json_to_instance": {
            "name": "_json_to_instance",
            "location": 43,
            "return": [
                "str",
                "Iterable[str]",
                "salon.models.Stylist",
                "int"
            ],
            "arguments": {
                "self": [],
                "json_dict": [
                    "str",
                    "Exception",
                    "tartare.core.models.Job",
                    "Dict[str, Any]"
                ]
            }
        }
    },
    "literate-lamp-master/literate_lamp/preprocess.py": {
        "clean_word": {
            "name": "clean_word",
            "location": 8,
            "return": [
                "Tuple[(str, str)]"
            ],
            "arguments": {
                "string": [
                    "str"
                ]
            }
        },
        "process_file": {
            "name": "process_file",
            "location": 16,
            "return": [
                "None"
            ],
            "arguments": {
                "input_file": [
                    "TextIO"
                ],
                "output_file": [
                    "TextIO"
                ]
            }
        }
    },
    "literate-lamp-master/literate_lamp/stats.py": {
        "flatten": {
            "name": "flatten",
            "location": 18,
            "return": [
                "List[T]"
            ],
            "arguments": {
                "l": [
                    "Iterable[List[T]]"
                ]
            }
        },
        "extract_field": {
            "name": "extract_field",
            "location": 70,
            "return": [
                "List[List[str]]"
            ],
            "arguments": {
                "field": [
                    "str"
                ],
                "instances": [
                    "list"
                ]
            }
        },
        "main": {
            "name": "main",
            "location": 81,
            "return": [
                "None"
            ],
            "arguments": {}
        },
        "TextStats.__init__": {
            "name": "__init__",
            "location": 30,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "instance_texts": [
                    "List[List[str]]"
                ]
            }
        },
        "TextStats.__repr__": {
            "name": "__repr__",
            "location": 37,
            "return": [
                "str"
            ],
            "arguments": {
                "self": []
            }
        },
        "DatasetStats.__init__": {
            "name": "__init__",
            "location": 48,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "instances": [
                    "list"
                ]
            }
        },
        "DatasetStats.__repr__": {
            "name": "__repr__",
            "location": 61,
            "return": [
                "str"
            ],
            "arguments": {
                "self": []
            }
        }
    },
    "literate-lamp-master/literate_lamp/train.py": {
        "make_prediction": {
            "name": "make_prediction",
            "location": 36,
            "return": [
                "deoplete.util.UserContext",
                "bool",
                "Optional[Union[str, Any]]"
            ],
            "arguments": {
                "model": [
                    "dict",
                    "jumeaux.models.FinalAddOnPayload",
                    "models.seat_group.SeatGroup"
                ],
                "reader": [
                    "int",
                    "Type[T]",
                    "typing.Type",
                    "Set[int]",
                    "set[int]"
                ],
                "verbose": [
                    "bool"
                ]
            }
        },
        "test_load": {
            "name": "test_load",
            "location": 61,
            "return": [
                "None"
            ],
            "arguments": {
                "build_model_fn": [
                    "Callable[([Any], Any)]"
                ],
                "reader": [
                    "typing.Mapping",
                    "dict[str, typing.Any]",
                    "Mapping[str, Any]",
                    "model.Model",
                    "Dict[str, Any]",
                    "keras.Model",
                    "keanu.Model"
                ],
                "save_path": [
                    "pathlib.Path"
                ],
                "original_prediction": [
                    "str",
                    "list[int]",
                    "allennlp.common.JsonDict",
                    "nevergrad.common.List[int]",
                    "numpy.ndarray",
                    "dict",
                    "List[int]",
                    "float"
                ],
                "cuda_device": [
                    "int"
                ]
            }
        },
        "run_model": {
            "name": "run_model",
            "location": 99,
            "return": [
                "None"
            ],
            "arguments": {}
        },
        "run_model.optimiser": {
            "name": "optimiser",
            "location": 109,
            "return": [
                "AdamW",
                "str",
                "numpy.ndarray",
                "bool"
            ],
            "arguments": {
                "model": [
                    "allennlp.models.Model",
                    "Type[T]",
                    "str",
                    "torch.nn.Module",
                    "models.cf_recommend_models.ICfRecommendationModel",
                    "List[Tuple[int, int, float]]",
                    "Model"
                ]
            }
        }
    },
    "literate-lamp-master/literate_lamp/util.py": {
        "visualise_model": {
            "name": "visualise_model",
            "location": 77,
            "return": [
                "None"
            ],
            "arguments": {
                "model": [
                    "TestModule.Model",
                    "Set[int]",
                    "int"
                ]
            }
        },
        "example_input": {
            "name": "example_input",
            "location": 90,
            "return": [
                "Tuple[(str, str, str, str)]"
            ],
            "arguments": {
                "index": [
                    "int"
                ]
            }
        },
        "is_cuda": {
            "name": "is_cuda",
            "location": 103,
            "return": [
                "bool"
            ],
            "arguments": {
                "model": [
                    "Model",
                    "models.Model"
                ]
            }
        },
        "train_val_test_split": {
            "name": "train_val_test_split",
            "location": 108,
            "return": [
                "Tuple[(Sequence, Sequence, Sequence)]"
            ],
            "arguments": {
                "dataset": [
                    "Sequence"
                ],
                "train_size": [
                    "float"
                ]
            }
        },
        "load_data": {
            "name": "load_data",
            "location": 128,
            "return": [
                "list"
            ],
            "arguments": {
                "reader": [
                    "None",
                    "str",
                    "Optional[str]",
                    "typing.TextIO",
                    "Optional[TextIO]",
                    "list[str]",
                    "Optional[pathlib.Path]",
                    "Path",
                    "Union[str, pathlib.Path]",
                    "Union[str, List[str]]"
                ],
                "data_path": [
                    "Optional[pathlib.Path]"
                ],
                "pre_processed_path": [
                    "Optional[pathlib.Path]"
                ]
            }
        },
        "train_model": {
            "name": "train_model",
            "location": 168,
            "return": [
                "allennlp.models.model.Model",
                "List[app.models.Question]",
                "app.models.Question"
            ],
            "arguments": {
                "build_model_fn": [
                    "Callable[([Any], Any)]"
                ],
                "train_data": [
                    "list"
                ],
                "val_data": [
                    "list"
                ],
                "test_data": [
                    "list"
                ],
                "save_path": [
                    "Optional[pathlib.Path]"
                ],
                "batch_size": [
                    "int"
                ],
                "num_epochs": [
                    "int"
                ],
                "optimiser_fn": [
                    "Optional[Callable[([Any], Any)]]"
                ],
                "grad_norm_clip": [
                    "float"
                ],
                "sorting_keys": [
                    "Optional[List[Tuple[(str, str)]]]"
                ],
                "cuda_device": [
                    "Union[(int, List[int])]"
                ]
            }
        },
        "get_preprocessed_name": {
            "name": "get_preprocessed_name",
            "location": 270,
            "return": [
                "str"
            ],
            "arguments": {
                "split_name": [
                    "str"
                ],
                "model": [
                    "str"
                ],
                "config": [
                    "str"
                ],
                "embedding": [
                    "str"
                ]
            }
        },
        "get_experiment_name": {
            "name": "get_experiment_name",
            "location": 276,
            "return": [
                "str"
            ],
            "arguments": {
                "model": [
                    "str"
                ],
                "config": [
                    "str"
                ],
                "embedding": [
                    "str"
                ],
                "name": [
                    "Optional[str]"
                ]
            }
        },
        "is_stopword": {
            "name": "is_stopword",
            "location": 290,
            "return": [
                "bool"
            ],
            "arguments": {
                "word": [
                    "str"
                ]
            }
        },
        "is_punctuation": {
            "name": "is_punctuation",
            "location": 299,
            "return": [
                "bool"
            ],
            "arguments": {
                "word": [
                    "str",
                    "Type",
                    "typing.Type"
                ]
            }
        },
        "get_term_frequency": {
            "name": "get_term_frequency",
            "location": 306,
            "return": [
                "float"
            ],
            "arguments": {
                "word": [
                    "str",
                    "int",
                    "dict[, ]",
                    "bytes",
                    "dict",
                    "bool"
                ]
            }
        },
        "clone_module": {
            "name": "clone_module",
            "location": 323,
            "return": [
                "bool",
                "Optional[str]",
                "Dict[str, Any]",
                "str"
            ],
            "arguments": {
                "module": [
                    "int",
                    "str",
                    "Optional[int]",
                    "None",
                    "Callable",
                    "typing.Callable[, ]",
                    "List[Tuple[int, int]]",
                    "list[tuple[typing.Union[int,int]]]"
                ],
                "num_clones": [
                    "int"
                ]
            }
        },
        "parse_cuda": {
            "name": "parse_cuda",
            "location": 331,
            "return": [
                "Union[(int, List[int])]"
            ],
            "arguments": {
                "cuda_str": [
                    "str"
                ]
            }
        },
        "tf2str": {
            "name": "tf2str",
            "location": 341,
            "return": [
                "str"
            ],
            "arguments": {
                "field": [
                    "str"
                ]
            }
        },
        "split_list": {
            "name": "split_list",
            "location": 350,
            "return": [
                "List[List[T]]"
            ],
            "arguments": {
                "list": [
                    "List[T]"
                ],
                "element": [
                    "T"
                ]
            }
        },
        "print_args": {
            "name": "print_args",
            "location": 364,
            "return": [
                "None"
            ],
            "arguments": {
                "args": [
                    "Dict[(str, Any)]"
                ]
            }
        },
        "DotDict.__init__": {
            "name": "__init__",
            "location": 47,
            "return": [
                "None"
            ],
            "arguments": {
                "self": []
            }
        },
        "DotDict.__getattr__": {
            "name": "__getattr__",
            "location": 59,
            "return": [
                "str",
                "Dict[str, str]",
                "Optional[str]"
            ],
            "arguments": {
                "self": [],
                "attr": [
                    "str"
                ]
            }
        },
        "DotDict.__setattr__": {
            "name": "__setattr__",
            "location": 62,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "key": [
                    "str"
                ],
                "value": [
                    "str",
                    "VT",
                    "KT"
                ]
            }
        },
        "DotDict.__setitem__": {
            "name": "__setitem__",
            "location": 65,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "key": [
                    "str"
                ],
                "value": [
                    "str",
                    "bool",
                    "set[]",
                    "set",
                    "T",
                    "dict[str, typing.Any]",
                    "Dict[str, Any]"
                ]
            }
        },
        "DotDict.__delattr__": {
            "name": "__delattr__",
            "location": 69,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "key": [
                    "str"
                ]
            }
        },
        "DotDict.__delitem__": {
            "name": "__delitem__",
            "location": 72,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "key": [
                    "str"
                ]
            }
        }
    },
    "literate-lamp-master/literate_lamp/__init__.py": {},
    "literate-lamp-master/literate_lamp/models/advanced_attention_bert.py": {
        "AdvancedAttentionBertClassifier.__init__": {
            "name": "__init__",
            "location": 23,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "bert_path": [
                    "pathlib.Path"
                ],
                "encoder": [
                    "str",
                    "bool",
                    "None",
                    "pathlib.Path",
                    "programl.models.lstm.lstm.Lstm",
                    "Optional[str]"
                ],
                "vocab": [
                    "str",
                    "allennlp.data.vocabulary.Vocabulary",
                    "list[str]",
                    "Optional[List[str]]",
                    "None"
                ],
                "hidden_dim": [
                    "int"
                ],
                "encoder_dropout": [
                    "float"
                ],
                "train_bert": [
                    "bool"
                ]
            }
        },
        "AdvancedAttentionBertClassifier.forward": {
            "name": "forward",
            "location": 61,
            "return": [
                "Dict[(str, Any)]"
            ],
            "arguments": {
                "self": [],
                "metadata": [
                    "Dict[(str, Any)]"
                ],
                "bert0": [
                    "Dict[(str, Any)]"
                ],
                "bert1": [
                    "Dict[(str, Any)]"
                ],
                "label": [
                    "str",
                    "None",
                    "bytes",
                    "Optional[List[str]]",
                    "bool",
                    "list[str]",
                    "Optional[float]",
                    "float"
                ]
            }
        }
    },
    "literate-lamp-master/literate_lamp/models/advanced_bert.py": {
        "AdvancedBertClassifier.__init__": {
            "name": "__init__",
            "location": 21,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "bert_path": [
                    "pathlib.Path"
                ],
                "encoder": [
                    "str",
                    "bool",
                    "int",
                    "pathlib.Path"
                ],
                "vocab": [
                    "str",
                    "allennlp.data.vocabulary.Vocabulary",
                    "list[str]",
                    "Optional[List[str]]",
                    "None"
                ],
                "hidden_dim": [
                    "int"
                ],
                "encoder_dropout": [
                    "float"
                ],
                "train_bert": [
                    "bool"
                ]
            }
        },
        "AdvancedBertClassifier.forward": {
            "name": "forward",
            "location": 55,
            "return": [
                "Dict[(str, Any)]"
            ],
            "arguments": {
                "self": [],
                "metadata": [
                    "Dict[(str, Any)]"
                ],
                "bert0": [
                    "Dict[(str, Any)]"
                ],
                "bert1": [
                    "Dict[(str, Any)]"
                ],
                "label": [
                    "str",
                    "None",
                    "bytes",
                    "Optional[List[str]]",
                    "bool",
                    "list[str]",
                    "Optional[float]",
                    "float"
                ]
            }
        }
    },
    "literate-lamp-master/literate_lamp/models/advanced_xlnet.py": {
        "AdvancedXLNetClassifier.__init__": {
            "name": "__init__",
            "location": 21,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "vocab": [
                    "bool",
                    "denite.util.Nvim",
                    "dict[str, typing.Any]",
                    "typing.Callable[A, bool]",
                    "deoplete.util.Nvim",
                    "Dict[str, Any]",
                    "Callable[[Any, Any], bool]"
                ],
                "word_embeddings": [
                    "int",
                    "float",
                    "List[allennlp.data.Instance]",
                    "list[allennlp.data.Instance]"
                ],
                "encoder": [
                    "int",
                    "str",
                    "salon.models.Stylist"
                ],
                "encoder_dropout": [
                    "float"
                ],
                "train_xlnet": [
                    "bool"
                ]
            }
        },
        "AdvancedXLNetClassifier.forward": {
            "name": "forward",
            "location": 45,
            "return": [
                "Dict[(str, Any)]"
            ],
            "arguments": {
                "self": [],
                "metadata": [
                    "Dict[(str, Any)]"
                ],
                "string0": [
                    "Dict[(str, Any)]"
                ],
                "string1": [
                    "Dict[(str, Any)]"
                ],
                "label": [
                    "None",
                    "str",
                    "Optional[str]",
                    "bool",
                    "dict[str, str]",
                    "Dict[str, str]",
                    "Optional[Iterable[str]]"
                ]
            }
        }
    },
    "literate-lamp-master/literate_lamp/models/attentive_reader.py": {
        "AttentiveReader.__init__": {
            "name": "__init__",
            "location": 35,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "word_embeddings": [
                    "float",
                    "int",
                    "List[float]",
                    "list[float]",
                    "type",
                    "typing.Type"
                ],
                "p_encoder": [
                    "bool",
                    "typing.Type",
                    "allennlp.modules.seq2vec_encoders.Seq2VecEncoder",
                    "typing.Mapping",
                    "type",
                    "None",
                    "Optional[Mapping]",
                    "int"
                ],
                "q_encoder": [
                    "float",
                    "str"
                ],
                "a_encoder": [
                    "Mapping[str, Tuple[float, float, float]]",
                    "typing.Mapping"
                ],
                "vocab": [
                    "str",
                    "int",
                    "allennlp.data.vocabulary.Vocabulary",
                    "tuple[typing.Union[int,int]]",
                    "Tuple[int, int]",
                    "torch.optim.Optimizer"
                ]
            }
        },
        "AttentiveReader.forward": {
            "name": "forward",
            "location": 67,
            "return": [
                "Dict[(str, Any)]"
            ],
            "arguments": {
                "self": [],
                "metadata": [
                    "Dict[(str, Any)]"
                ],
                "passage": [
                    "Dict[(str, Any)]"
                ],
                "question": [
                    "Dict[(str, Any)]"
                ],
                "answer0": [
                    "Dict[(str, Any)]"
                ],
                "answer1": [
                    "Dict[(str, Any)]"
                ],
                "label": [
                    "Optional[Set[int]]",
                    "str",
                    "None",
                    "set[int]"
                ]
            }
        }
    },
    "literate-lamp-master/literate_lamp/models/baseline.py": {
        "BaselineClassifier.__init__": {
            "name": "__init__",
            "location": 27,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "word_embeddings": [
                    "int",
                    "float",
                    "List[allennlp.data.Instance]",
                    "list[allennlp.data.Instance]"
                ],
                "encoder": [
                    "int",
                    "float",
                    "bool"
                ],
                "vocab": [
                    "bool",
                    "models.LTI1p3Provider",
                    "str"
                ]
            }
        },
        "BaselineClassifier.forward": {
            "name": "forward",
            "location": 58,
            "return": [
                "Dict[(str, Any)]"
            ],
            "arguments": {
                "self": [],
                "metadata": [
                    "Dict[(str, Any)]"
                ],
                "passage": [
                    "Dict[(str, Any)]"
                ],
                "question": [
                    "Dict[(str, Any)]"
                ],
                "answer0": [
                    "Dict[(str, Any)]"
                ],
                "answer1": [
                    "Dict[(str, Any)]"
                ],
                "label": [
                    "Optional[Set[int]]",
                    "str",
                    "None",
                    "set[int]"
                ]
            }
        }
    },
    "literate-lamp-master/literate_lamp/models/base_model.py": {
        "BaseModel.__init__": {
            "name": "__init__",
            "location": 17,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "vocab": [
                    "bool",
                    "Dict[str, Any]",
                    "dict[str, typing.Any]",
                    "Callable[[Any, Any], bool]",
                    "typing.Callable[Any,Any, bool]"
                ]
            }
        },
        "BaseModel.get_metrics": {
            "name": "get_metrics",
            "location": 29,
            "return": [
                "Dict[(str, float)]"
            ],
            "arguments": {
                "self": [],
                "reset": [
                    "bool"
                ]
            }
        }
    },
    "literate-lamp-master/literate_lamp/models/dcmn.py": {
        "Dcmn.__init__": {
            "name": "__init__",
            "location": 18,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "word_embeddings": [
                    "list",
                    "int",
                    "bool",
                    "List[int]",
                    "Callable"
                ],
                "vocab": [
                    "bool",
                    "Dict[str, Any]",
                    "dict[str, typing.Any]",
                    "Callable[[Any, Any], bool]",
                    "typing.Callable[Any,Any, bool]"
                ],
                "embedding_dropout": [
                    "float"
                ]
            }
        },
        "Dcmn._forward_internal": {
            "name": "_forward_internal",
            "location": 55,
            "return": [
                "int",
                "str",
                "Callable",
                "models.Question",
                "dict[str, typing.Any]",
                "dict[str, float]",
                "dict[int, str]",
                "typing.Pattern"
            ],
            "arguments": {
                "self": [],
                "passage": [
                    "Dict[(str, Any)]"
                ],
                "question": [
                    "Dict[(str, Any)]"
                ],
                "answer": [
                    "Dict[(str, Any)]"
                ]
            }
        },
        "Dcmn.forward": {
            "name": "forward",
            "location": 107,
            "return": [
                "Dict[(str, Any)]"
            ],
            "arguments": {
                "self": [],
                "metadata": [
                    "Dict[(str, Any)]"
                ],
                "passage": [
                    "Dict[(str, Any)]"
                ],
                "question": [
                    "Dict[(str, Any)]"
                ],
                "answer0": [
                    "Dict[(str, Any)]"
                ],
                "answer1": [
                    "Dict[(str, Any)]"
                ],
                "label": [
                    "Optional[Set[int]]",
                    "str",
                    "None",
                    "set[int]"
                ]
            }
        }
    },
    "literate-lamp-master/literate_lamp/models/hierarchical_attention_network.py": {
        "HierarchicalAttentionNetwork.__init__": {
            "name": "__init__",
            "location": 24,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "word_embeddings": [
                    "int",
                    "float",
                    "List[allennlp.data.Instance]",
                    "list[allennlp.data.Instance]"
                ],
                "sentence_encoder": [
                    "str",
                    "zam_repondeur.models.Lecture",
                    "torch.Tensor"
                ],
                "document_encoder": [
                    "zam_repondeur.models.Lecture",
                    "bool"
                ],
                "vocab": [
                    "bool",
                    "denite.util.Nvim",
                    "dict[str, typing.Any]",
                    "typing.Callable[Any,Any, bool]",
                    "deoplete.util.Nvim",
                    "Dict[str, Any]",
                    "Callable[[Any, Any], bool]"
                ],
                "encoder_dropout": [
                    "float"
                ]
            }
        },
        "HierarchicalAttentionNetwork.forward": {
            "name": "forward",
            "location": 63,
            "return": [
                "Dict[(str, Any)]"
            ],
            "arguments": {
                "self": [],
                "metadata": [
                    "Dict[(str, Any)]"
                ],
                "bert0": [
                    "Dict[(str, Any)]"
                ],
                "bert1": [
                    "Dict[(str, Any)]"
                ],
                "label": [
                    "str",
                    "None",
                    "bytes",
                    "Optional[List[str]]",
                    "bool",
                    "list[str]",
                    "Optional[float]",
                    "float"
                ]
            }
        }
    },
    "literate-lamp-master/literate_lamp/models/hierarchical_bert.py": {
        "HierarchicalBert.__init__": {
            "name": "__init__",
            "location": 21,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "bert_path": [
                    "pathlib.Path"
                ],
                "sentence_encoder": [
                    "dict",
                    "bool",
                    "dict[, ]"
                ],
                "document_encoder": [
                    "str",
                    "allennlp.common.util.JsonDict",
                    "Dict[str, allennlp.commands.subcommand.Subcommand]",
                    "bool"
                ],
                "vocab": [
                    "str",
                    "allennlp.data.vocabulary.Vocabulary",
                    "list[str]",
                    "Optional[List[str]]",
                    "None"
                ],
                "encoder_dropout": [
                    "float"
                ],
                "train_bert": [
                    "bool"
                ]
            }
        },
        "HierarchicalBert.forward": {
            "name": "forward",
            "location": 50,
            "return": [
                "Dict[(str, Any)]"
            ],
            "arguments": {
                "self": [],
                "metadata": [
                    "Dict[(str, Any)]"
                ],
                "bert0": [
                    "Dict[(str, Any)]"
                ],
                "bert1": [
                    "Dict[(str, Any)]"
                ],
                "label": [
                    "str",
                    "None",
                    "bytes",
                    "Optional[List[str]]",
                    "bool",
                    "list[str]",
                    "Optional[float]",
                    "float"
                ]
            }
        }
    },
    "literate-lamp-master/literate_lamp/models/relational_han.py": {
        "RelationalHan.__init__": {
            "name": "__init__",
            "location": 26,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "word_embeddings": [
                    "int",
                    "float",
                    "List[allennlp.data.Instance]",
                    "list[allennlp.data.Instance]"
                ],
                "sentence_encoder": [
                    "float"
                ],
                "document_encoder": [
                    "zam_repondeur.models.Lecture",
                    "torch.Tensor",
                    "Sequence['cirq.Qid']",
                    "float"
                ],
                "relation_encoder": [
                    "float"
                ],
                "document_relation_encoder": [
                    "int",
                    "dict[str, typing.Any]",
                    "Dict[str, Any]",
                    "str",
                    "Union[str, int]",
                    "bool"
                ],
                "vocab": [
                    "bool",
                    "denite.util.Nvim",
                    "dict[str, typing.Any]",
                    "typing.Callable[Any,Any, bool]",
                    "deoplete.util.Nvim",
                    "Dict[str, Any]",
                    "Callable[[Any, Any], bool]"
                ],
                "encoder_dropout": [
                    "float"
                ],
                "ffn_dropout": [
                    "float"
                ]
            }
        },
        "RelationalHan._forward_internal": {
            "name": "_forward_internal",
            "location": 76,
            "return": [
                "int",
                "Callable",
                "models.Question",
                "str"
            ],
            "arguments": {
                "self": [],
                "bert": [
                    "Dict[(str, Any)]"
                ],
                "relations": [
                    "Dict[(str, Any)]"
                ]
            }
        },
        "RelationalHan.forward": {
            "name": "forward",
            "location": 103,
            "return": [
                "Dict[(str, Any)]"
            ],
            "arguments": {
                "self": [],
                "metadata": [
                    "Dict[(str, Any)]"
                ],
                "bert0": [
                    "Dict[(str, Any)]"
                ],
                "bert1": [
                    "Dict[(str, Any)]"
                ],
                "p_a0_rel": [
                    "Dict[(str, Any)]"
                ],
                "p_a1_rel": [
                    "Dict[(str, Any)]"
                ],
                "label": [
                    "None",
                    "str",
                    "Optional[str]",
                    "int",
                    "typing.Callable[str, bool]",
                    "Optional[Callable[[str], bool]]",
                    "typing.Sequence[typing.Any]",
                    "Optional[Sequence[Any]]",
                    "float",
                    "Optional[float]",
                    "Optional[int]"
                ]
            }
        }
    },
    "literate-lamp-master/literate_lamp/models/relational_transformer_model.py": {
        "RelationalTransformerModel.__init__": {
            "name": "__init__",
            "location": 18,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "word_embeddings": [
                    "int",
                    "float",
                    "List[allennlp.data.Instance]",
                    "list[allennlp.data.Instance]"
                ],
                "sentence_encoder": [
                    "float"
                ],
                "relation_sentence_encoder": [
                    "int",
                    "float",
                    "bool"
                ],
                "relational_encoder": [
                    "bool",
                    "zam_repondeur.models.Lecture"
                ],
                "rel_embeddings": [
                    "int",
                    "float",
                    "bool"
                ],
                "vocab": [
                    "bool",
                    "denite.util.Nvim",
                    "dict[str, typing.Any]",
                    "typing.Callable[Any,Any, bool]",
                    "deoplete.util.Nvim",
                    "Dict[str, Any]",
                    "Callable[[Any, Any], bool]"
                ],
                "encoder_dropout": [
                    "float"
                ]
            }
        },
        "RelationalTransformerModel.forward": {
            "name": "forward",
            "location": 64,
            "return": [
                "Dict[(str, Any)]"
            ],
            "arguments": {
                "self": [],
                "metadata": [
                    "Dict[(str, Any)]"
                ],
                "bert0": [
                    "Dict[(str, Any)]"
                ],
                "bert1": [
                    "Dict[(str, Any)]"
                ],
                "p_a0_rel": [
                    "Dict[(str, Any)]"
                ],
                "p_a1_rel": [
                    "Dict[(str, Any)]"
                ],
                "label": [
                    "None",
                    "str",
                    "Optional[str]",
                    "int",
                    "typing.Callable[str, bool]",
                    "Optional[Callable[[str], bool]]",
                    "typing.Sequence[typing.Any]",
                    "Optional[Sequence[Any]]",
                    "float",
                    "Optional[float]",
                    "Optional[int]"
                ]
            }
        }
    },
    "literate-lamp-master/literate_lamp/models/relational_xlnet.py": {
        "RelationalXL.__init__": {
            "name": "__init__",
            "location": 26,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "word_embeddings": [
                    "int",
                    "str",
                    "typing.Container",
                    "Container[str]",
                    "bool"
                ],
                "text_encoder": [
                    "str",
                    "int",
                    "TextIO"
                ],
                "relation_encoder": [
                    "str",
                    "Optional[int]",
                    "Optional[str]",
                    "int",
                    "bool"
                ],
                "vocab": [
                    "str"
                ],
                "encoder_dropout": [
                    "float"
                ]
            }
        },
        "RelationalXL._forward_internal": {
            "name": "_forward_internal",
            "location": 58,
            "return": [
                "int",
                "Callable",
                "models.Question",
                "str"
            ],
            "arguments": {
                "self": [],
                "text": [
                    "Dict[(str, Any)]"
                ],
                "relations": [
                    "Dict[(str, Any)]"
                ]
            }
        },
        "RelationalXL.forward": {
            "name": "forward",
            "location": 89,
            "return": [
                "Dict[(str, Any)]"
            ],
            "arguments": {
                "self": [],
                "metadata": [
                    "Dict[(str, Any)]"
                ],
                "string0": [
                    "Dict[(str, Any)]"
                ],
                "string1": [
                    "Dict[(str, Any)]"
                ],
                "rel0": [
                    "Dict[(str, Any)]"
                ],
                "rel1": [
                    "Dict[(str, Any)]"
                ],
                "label": [
                    "bytes",
                    "None",
                    "Optional[Set[int]]",
                    "set[int]"
                ]
            }
        }
    },
    "literate-lamp-master/literate_lamp/models/simple_bert.py": {
        "SimpleBertClassifier.__init__": {
            "name": "__init__",
            "location": 21,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "bert_path": [
                    "pathlib.Path"
                ],
                "vocab": [
                    "str",
                    "allennlp.data.vocabulary.Vocabulary",
                    "list[str]",
                    "Optional[List[str]]",
                    "None"
                ],
                "train_bert": [
                    "bool"
                ]
            }
        },
        "SimpleBertClassifier.forward": {
            "name": "forward",
            "location": 43,
            "return": [
                "Dict[(str, Any)]"
            ],
            "arguments": {
                "self": [],
                "metadata": [
                    "Dict[(str, Any)]"
                ],
                "bert0": [
                    "Dict[(str, Any)]"
                ],
                "bert1": [
                    "Dict[(str, Any)]"
                ],
                "label": [
                    "str",
                    "None",
                    "bytes",
                    "Optional[List[str]]",
                    "bool",
                    "list[str]",
                    "Optional[float]",
                    "float"
                ]
            }
        }
    },
    "literate-lamp-master/literate_lamp/models/simple_trian.py": {
        "SimpleTrian.__init__": {
            "name": "__init__",
            "location": 37,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "word_embeddings": [
                    "int",
                    "List[int]"
                ],
                "rel_embeddings": [
                    "float",
                    "bool",
                    "static_frame.core.util.UFunc",
                    "int"
                ],
                "p_encoder": [
                    "float",
                    "int",
                    "str",
                    "typing.Type",
                    "allennlp.modules.seq2vec_encoders.Seq2VecEncoder",
                    "type"
                ],
                "q_encoder": [
                    "float",
                    "Optional[Union[float, Any]]",
                    "str",
                    "typing.Any",
                    "Optional[Iterable[\"Outcome\"]]",
                    "None",
                    "typing.Iterable[O]"
                ],
                "a_encoder": [
                    "int",
                    "Mapping[str, Tuple[float, float, float]]",
                    "typing.Mapping"
                ],
                "vocab": [
                    "bool",
                    "str",
                    "allennlp.data.vocabulary.Vocabulary",
                    "int"
                ],
                "embedding_dropout": [
                    "float"
                ],
                "encoder_dropout": [
                    "float"
                ]
            }
        },
        "SimpleTrian.forward": {
            "name": "forward",
            "location": 97,
            "return": [
                "Dict[(str, Any)]"
            ],
            "arguments": {
                "self": [],
                "metadata": [
                    "Dict[(str, Any)]"
                ],
                "passage": [
                    "Dict[(str, Any)]"
                ],
                "question": [
                    "Dict[(str, Any)]"
                ],
                "answer0": [
                    "Dict[(str, Any)]"
                ],
                "answer1": [
                    "Dict[(str, Any)]"
                ],
                "p_q_rel": [
                    "Dict[(str, Any)]"
                ],
                "p_a0_rel": [
                    "Dict[(str, Any)]"
                ],
                "p_a1_rel": [
                    "Dict[(str, Any)]"
                ],
                "label": [
                    "None",
                    "str",
                    "Optional[str]",
                    "typing.Any",
                    "Optional[Union[str, Any]]",
                    "bool",
                    "Optional[bool]",
                    "set[int]",
                    "Optional[Set[int]]",
                    "Optional[int]"
                ]
            }
        }
    },
    "literate-lamp-master/literate_lamp/models/simple_xlnet.py": {
        "SimpleXLNetClassifier.__init__": {
            "name": "__init__",
            "location": 20,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "vocab": [
                    "str",
                    "allennlp.data.vocabulary.Vocabulary",
                    "list[str]",
                    "Optional[List[str]]",
                    "None"
                ],
                "config_path": [
                    "pathlib.Path"
                ],
                "model_path": [
                    "pathlib.Path"
                ],
                "train_xlnet": [
                    "bool"
                ]
            }
        },
        "SimpleXLNetClassifier.forward": {
            "name": "forward",
            "location": 45,
            "return": [
                "Dict[(str, Any)]"
            ],
            "arguments": {
                "self": [],
                "metadata": [
                    "Dict[(str, Any)]"
                ],
                "string0": [
                    "Dict[(str, Any)]"
                ],
                "string1": [
                    "Dict[(str, Any)]"
                ],
                "label": [
                    "None",
                    "str",
                    "Optional[str]",
                    "bool",
                    "dict[str, str]",
                    "Dict[str, str]",
                    "Optional[Iterable[str]]"
                ]
            }
        }
    },
    "literate-lamp-master/literate_lamp/models/trian.py": {
        "Trian.__init__": {
            "name": "__init__",
            "location": 36,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "word_embeddings": [
                    "list",
                    "int",
                    "bool",
                    "List[int]",
                    "Callable"
                ],
                "pos_embeddings": [
                    "int",
                    "float",
                    "bool"
                ],
                "ner_embeddings": [
                    "int",
                    "float",
                    "bool"
                ],
                "rel_embeddings": [
                    "int",
                    "float",
                    "bool"
                ],
                "p_encoder": [
                    "int",
                    "float",
                    "allennlp.modules.seq2seq_encoders.Seq2SeqEncoder",
                    "bool"
                ],
                "q_encoder": [
                    "str",
                    "int"
                ],
                "a_encoder": [
                    "int"
                ],
                "vocab": [
                    "bool",
                    "denite.util.Nvim",
                    "dict[str, typing.Any]",
                    "typing.Callable[Any,Any, bool]",
                    "deoplete.util.Nvim",
                    "Dict[str, Any]",
                    "Callable[[Any, Any], bool]"
                ],
                "embedding_dropout": [
                    "float"
                ],
                "encoder_dropout": [
                    "float"
                ]
            }
        },
        "Trian.forward": {
            "name": "forward",
            "location": 94,
            "return": [
                "Dict[(str, Any)]"
            ],
            "arguments": {
                "self": [],
                "metadata": [
                    "Dict[(str, Any)]"
                ],
                "passage": [
                    "Dict[(str, Any)]"
                ],
                "question": [
                    "Dict[(str, Any)]"
                ],
                "answer0": [
                    "Dict[(str, Any)]"
                ],
                "answer1": [
                    "Dict[(str, Any)]"
                ],
                "passage_pos": [
                    "Dict[(str, Any)]"
                ],
                "passage_ner": [
                    "Dict[(str, Any)]"
                ],
                "question_pos": [
                    "Dict[(str, Any)]"
                ],
                "p_q_rel": [
                    "Dict[(str, Any)]"
                ],
                "p_a0_rel": [
                    "Dict[(str, Any)]"
                ],
                "p_a1_rel": [
                    "Dict[(str, Any)]"
                ],
                "hc_feat": [
                    "Optional[float]",
                    "float",
                    "str",
                    "None",
                    "List[str]",
                    "Dict[int, Set[int]]",
                    "list[str]",
                    "dict[int, set[int]]"
                ],
                "label": [
                    "Optional[Set[int]]",
                    "None",
                    "set[int]"
                ]
            }
        }
    },
    "literate-lamp-master/literate_lamp/models/util.py": {
        "seq_over_seq": {
            "name": "seq_over_seq",
            "location": 10,
            "return": [
                "int",
                "Callable",
                "str"
            ],
            "arguments": {
                "encoder": [
                    "str",
                    "float",
                    "int"
                ],
                "sentences": [
                    "bool",
                    "float"
                ],
                "masks": [
                    "str",
                    "int",
                    "None",
                    "torch.Tensor",
                    "Optional[str]"
                ]
            }
        },
        "hierarchical_seq_over_seq": {
            "name": "hierarchical_seq_over_seq",
            "location": 30,
            "return": [
                "int",
                "Callable",
                "str"
            ],
            "arguments": {
                "encoder": [
                    "str",
                    "float",
                    "int"
                ],
                "sentences": [
                    "int"
                ],
                "masks": [
                    "list[]",
                    "list",
                    "tuple[typing.Union[int,int,int]]",
                    "OrderedDict",
                    "list[str]",
                    "Tuple[int, int, int]",
                    "List[str]"
                ]
            }
        },
        "attention_over_sequence": {
            "name": "attention_over_sequence",
            "location": 46,
            "return": [
                "int",
                "Callable",
                "float",
                "str",
                "dict[str, int]",
                "list[int]"
            ],
            "arguments": {
                "attention": [
                    "int",
                    "str",
                    "list[]",
                    "list",
                    "torch.Tensor"
                ],
                "sequence": [
                    "str",
                    "list[]",
                    "list",
                    "int"
                ],
                "vector": [
                    "int",
                    "str",
                    "list[]",
                    "list",
                    "torch.Tensor"
                ]
            }
        },
        "initalise_weights": {
            "name": "initalise_weights",
            "location": 58,
            "return": [
                "None"
            ],
            "arguments": {
                "init": [
                    "Callable[([Any], None)]"
                ],
                "module": [
                    "List[str]",
                    "int",
                    "str"
                ]
            }
        }
    },
    "literate-lamp-master/literate_lamp/models/zero_trian.py": {
        "ZeroTrian.__init__": {
            "name": "__init__",
            "location": 38,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "word_embeddings": [
                    "int"
                ],
                "p_encoder": [
                    "bool",
                    "typing.Type",
                    "allennlp.modules.seq2vec_encoders.Seq2VecEncoder",
                    "typing.Mapping",
                    "type",
                    "None",
                    "Optional[Mapping]",
                    "int"
                ],
                "q_encoder": [
                    "float",
                    "str"
                ],
                "a_encoder": [
                    "Mapping[str, Tuple[float, float, float]]",
                    "typing.Mapping",
                    "Union[\"SAONegotiator\", Collection[\"SAONegotiator\"]]",
                    "typing.Collection"
                ],
                "vocab": [
                    "str",
                    "int",
                    "allennlp.data.vocabulary.Vocabulary",
                    "tuple[typing.Union[int,int]]",
                    "Tuple[int, int]",
                    "torch.optim.Optimizer"
                ],
                "embedding_dropout": [
                    "float"
                ],
                "encoder_dropout": [
                    "float"
                ]
            }
        },
        "ZeroTrian.forward": {
            "name": "forward",
            "location": 96,
            "return": [
                "Dict[(str, Any)]"
            ],
            "arguments": {
                "self": [],
                "metadata": [
                    "Dict[(str, Any)]"
                ],
                "passage": [
                    "Dict[(str, Any)]"
                ],
                "question": [
                    "Dict[(str, Any)]"
                ],
                "answer0": [
                    "Dict[(str, Any)]"
                ],
                "answer1": [
                    "Dict[(str, Any)]"
                ],
                "label": [
                    "Optional[Set[int]]",
                    "str",
                    "None",
                    "set[int]"
                ]
            }
        }
    },
    "literate-lamp-master/literate_lamp/models/__init__.py": {},
    "literate-lamp-master/literate_lamp/models/dmn/answer_module.py": {
        "AnswerModule.__init__": {
            "name": "__init__",
            "location": 14,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "word_embeddings": [
                    "int",
                    "float",
                    "List[allennlp.data.Instance]",
                    "list[allennlp.data.Instance]"
                ],
                "encoder": [
                    "int",
                    "float",
                    "bool"
                ],
                "embedding_dropout": [
                    "float"
                ],
                "encoder_dropout": [
                    "float"
                ]
            }
        },
        "AnswerModule.get_output_dim": {
            "name": "get_output_dim",
            "location": 28,
            "return": [
                "int"
            ],
            "arguments": {
                "self": []
            }
        },
        "AnswerModule.forward": {
            "name": "forward",
            "location": 32,
            "return": [
                "int",
                "Callable",
                "models.Question",
                "str"
            ],
            "arguments": {
                "self": [],
                "answer": [
                    "str",
                    "bool",
                    "typing.Mapping",
                    "list[str]",
                    "Mapping[str, Any]",
                    "models.Movie",
                    "paradigm.models.Base",
                    "List[str]"
                ]
            }
        }
    },
    "literate-lamp-master/literate_lamp/models/dmn/input_module.py": {
        "InputModule.__init__": {
            "name": "__init__",
            "location": 14,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "word_embeddings": [
                    "int",
                    "float",
                    "List[allennlp.data.Instance]",
                    "list[allennlp.data.Instance]"
                ],
                "sentence_encoder": [
                    "int",
                    "float",
                    "bool"
                ],
                "document_encoder": [
                    "int",
                    "float",
                    "bool"
                ],
                "embedding_dropout": [
                    "float"
                ],
                "encoder_dropout": [
                    "float"
                ]
            }
        },
        "InputModule.get_output_dim": {
            "name": "get_output_dim",
            "location": 32,
            "return": [
                "int"
            ],
            "arguments": {
                "self": []
            }
        },
        "InputModule.forward": {
            "name": "forward",
            "location": 36,
            "return": [
                "int",
                "Callable",
                "models.Question",
                "str"
            ],
            "arguments": {
                "self": [],
                "sentences": [
                    "float",
                    "str",
                    "Optional[float]",
                    "int",
                    "None",
                    "List[str]",
                    "list[str]"
                ]
            }
        }
    },
    "literate-lamp-master/literate_lamp/models/dmn/memory_module.py": {
        "MemoryModule.__init__": {
            "name": "__init__",
            "location": 14,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "hidden_dim": [
                    "int"
                ],
                "num_hops": [
                    "int"
                ],
                "dropout": [
                    "float"
                ]
            }
        },
        "MemoryModule.get_output_dim": {
            "name": "get_output_dim",
            "location": 39,
            "return": [
                "int"
            ],
            "arguments": {
                "self": []
            }
        },
        "MemoryModule.get_gate": {
            "name": "get_gate",
            "location": 42,
            "return": [
                "str",
                "bool"
            ],
            "arguments": {
                "self": [],
                "facts": [
                    "list",
                    "list[]",
                    "float",
                    "List[Tuple[int, int]]",
                    "str",
                    "list[tuple[typing.Union[int,int]]]",
                    "List[Dict]",
                    "list[dict[, ]]"
                ],
                "question": [
                    "str",
                    "Optional[str]",
                    "None",
                    "List[str]",
                    "list[str]"
                ],
                "answer": [
                    "str",
                    "int",
                    "static_frame.core.util.DepthLevelSpecifier",
                    "typing.Callable[str, bool]",
                    "Optional[Callable[[str], bool]]",
                    "None"
                ],
                "prev_mem": [
                    "bool",
                    "str",
                    "int"
                ]
            }
        },
        "MemoryModule.forward": {
            "name": "forward",
            "location": 77,
            "return": [
                "int",
                "Callable",
                "models.Question",
                "str"
            ],
            "arguments": {
                "self": [],
                "facts": [
                    "dict[, ]",
                    "dict",
                    "bool",
                    "list[str]",
                    "models.User",
                    "bytes",
                    "str",
                    "List[str]",
                    "typing.IO",
                    "int",
                    "IO"
                ],
                "question": [
                    "str",
                    "list[torch.Tensor]",
                    "x509_pki.models.Certificate",
                    "dict[, ]",
                    "List[torch.Tensor]",
                    "dict"
                ],
                "answer": [
                    "str",
                    "list[torch.Tensor]",
                    "x509_pki.models.Certificate",
                    "dict[, ]",
                    "List[torch.Tensor]",
                    "dict"
                ],
                "prev_mem": [
                    "str",
                    "list[torch.Tensor]",
                    "x509_pki.models.Certificate",
                    "dict[, ]",
                    "List[torch.Tensor]",
                    "dict"
                ],
                "hop": [
                    "complex",
                    "bytes",
                    "float",
                    "bool",
                    "Optional[str]",
                    "str",
                    "None"
                ]
            }
        }
    },
    "literate-lamp-master/literate_lamp/models/dmn/model.py": {
        "_assert_equal": {
            "name": "_assert_equal",
            "location": 128,
            "return": [
                "None"
            ],
            "arguments": {
                "a": [
                    "int",
                    "torch.nn.Module",
                    "torch.Tensor"
                ],
                "b": [
                    "int",
                    "Callable[[T], None]"
                ]
            }
        },
        "Dmn.__init__": {
            "name": "__init__",
            "location": 39,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "word_embeddings": [
                    "int",
                    "float",
                    "bool"
                ],
                "sentence_encoder": [
                    "str",
                    "int",
                    "Sequence[int]",
                    "allennlp.modules.seq2vec_encoders.Seq2VecEncoder"
                ],
                "document_encoder": [
                    "int",
                    "float",
                    "bool"
                ],
                "question_encoder": [
                    "int",
                    "float",
                    "bool"
                ],
                "answer_encoder": [
                    "int",
                    "float",
                    "bool"
                ],
                "passes": [
                    "int"
                ],
                "vocab": [
                    "str",
                    "int",
                    "allennlp.data.vocabulary.Vocabulary",
                    "bool",
                    "torch.optim.Optimizer"
                ],
                "embedding_dropout": [
                    "float"
                ],
                "encoder_dropout": [
                    "float"
                ]
            }
        },
        "Dmn.forward": {
            "name": "forward",
            "location": 88,
            "return": [
                "Dict[(str, Any)]"
            ],
            "arguments": {
                "self": [],
                "metadata": [
                    "Dict[(str, Any)]"
                ],
                "sentences": [
                    "Dict[(str, Any)]"
                ],
                "question": [
                    "Dict[(str, Any)]"
                ],
                "answer0": [
                    "Dict[(str, Any)]"
                ],
                "answer1": [
                    "Dict[(str, Any)]"
                ],
                "label": [
                    "Optional[Set[int]]",
                    "str",
                    "None",
                    "set[int]"
                ]
            }
        }
    },
    "literate-lamp-master/literate_lamp/models/dmn/output_module.py": {
        "OutputModule.__init__": {
            "name": "__init__",
            "location": 14,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "memory_size": [
                    "int"
                ],
                "answer_size": [
                    "int"
                ],
                "num_labels": [
                    "int"
                ]
            }
        },
        "OutputModule.get_input_dim": {
            "name": "get_input_dim",
            "location": 22,
            "return": [
                "int"
            ],
            "arguments": {
                "self": []
            }
        },
        "OutputModule.get_output_dim": {
            "name": "get_output_dim",
            "location": 25,
            "return": [
                "int"
            ],
            "arguments": {
                "self": []
            }
        },
        "OutputModule.forward": {
            "name": "forward",
            "location": 29,
            "return": [
                "int",
                "Callable",
                "models.Question",
                "str"
            ],
            "arguments": {
                "self": [],
                "memory": [
                    "str",
                    "Dict[str, int]"
                ],
                "answer": [
                    "str",
                    "Dict[str, int]",
                    "dict[str, int]"
                ]
            }
        }
    },
    "literate-lamp-master/literate_lamp/models/dmn/question_module.py": {
        "QuestionModule.__init__": {
            "name": "__init__",
            "location": 14,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "word_embeddings": [
                    "int",
                    "float",
                    "List[allennlp.data.Instance]",
                    "list[allennlp.data.Instance]"
                ],
                "encoder": [
                    "int",
                    "float",
                    "bool"
                ],
                "embedding_dropout": [
                    "float"
                ],
                "encoder_dropout": [
                    "float"
                ]
            }
        },
        "QuestionModule.get_output_dim": {
            "name": "get_output_dim",
            "location": 28,
            "return": [
                "int"
            ],
            "arguments": {
                "self": []
            }
        },
        "QuestionModule.forward": {
            "name": "forward",
            "location": 32,
            "return": [
                "int",
                "Callable",
                "models.Question",
                "str"
            ],
            "arguments": {
                "self": [],
                "question": [
                    "Any",
                    "typing.Any"
                ]
            }
        }
    },
    "literate-lamp-master/literate_lamp/models/dmn/__init__.py": {},
    "literate-lamp-master/literate_lamp/modules/attention_gru.py": {
        "attention_gru": {
            "name": "attention_gru",
            "location": 87,
            "return": [
                "int",
                "List[int]",
                "float",
                "List[dict]",
                "dict"
            ],
            "arguments": {
                "input_dim": [
                    "int"
                ],
                "output_dim": [
                    "int"
                ],
                "num_layers": [
                    "int"
                ],
                "bidirectional": [
                    "bool"
                ],
                "dropout": [
                    "float"
                ]
            }
        },
        "AttentionGRUCell.__init__": {
            "name": "__init__",
            "location": 9,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "input_dim": [
                    "int"
                ],
                "hidden_size": [
                    "int"
                ]
            }
        },
        "AttentionGRUCell.forward": {
            "name": "forward",
            "location": 22,
            "return": [
                "int",
                "torch.Tensor",
                "List[int]",
                "float"
            ],
            "arguments": {
                "self": [],
                "inputs": [],
                "previous_state": [],
                "gate": []
            }
        },
        "AttentionGRU.__init__": {
            "name": "__init__",
            "location": 47,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "input_dim": [
                    "int"
                ],
                "output_dim": [
                    "int"
                ]
            }
        },
        "AttentionGRU.forward": {
            "name": "forward",
            "location": 53,
            "return": [
                "Callable"
            ],
            "arguments": {
                "self": [],
                "inputs": [
                    "list[torch.Tensor]",
                    "torch.Tensor",
                    "List[torch.Tensor]"
                ],
                "gate": [
                    "BaseException"
                ]
            }
        },
        "AttentionGRU.get_input_dim": {
            "name": "get_input_dim",
            "location": 79,
            "return": [
                "str",
                "bool"
            ],
            "arguments": {
                "self": []
            }
        },
        "AttentionGRU.get_output_dim": {
            "name": "get_output_dim",
            "location": 83,
            "return": [
                "str",
                "Set[str]",
                "Sequence[int]",
                "bool"
            ],
            "arguments": {
                "self": []
            }
        }
    },
    "literate-lamp-master/literate_lamp/modules/position_encoder.py": {
        "position_encoder": {
            "name": "position_encoder",
            "location": 59,
            "return": [
                "PositionEncoder",
                "int",
                "List[int]",
                "float",
                "List[dict]",
                "dict"
            ],
            "arguments": {
                "input_dim": [
                    "int"
                ],
                "output_dim": [
                    "int"
                ],
                "num_layers": [
                    "int"
                ],
                "bidirectional": [
                    "bool"
                ],
                "dropout": [
                    "float"
                ]
            }
        },
        "PositionEncoder.__init__": {
            "name": "__init__",
            "location": 9,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "input_dim": [
                    "int"
                ],
                "output_dim": [
                    "int"
                ]
            }
        },
        "PositionEncoder.position_matrix": {
            "name": "position_matrix",
            "location": 15,
            "return": [
                "int",
                "Callable",
                "str"
            ],
            "arguments": {
                "self": [],
                "seq_len": [
                    "int"
                ]
            }
        },
        "PositionEncoder.forward": {
            "name": "forward",
            "location": 24,
            "return": [
                "str",
                "int",
                "Optional[str]"
            ],
            "arguments": {
                "self": [],
                "inputs": [
                    "torch.Tensor",
                    "int",
                    "torch.ByteTensor",
                    "Optional[torch.Tensor]"
                ],
                "mask": [
                    "Optional[int]",
                    "None",
                    "float",
                    "int",
                    "str",
                    "Optional[Any]",
                    "Optional[Mapping[int, complex]]",
                    "typing.Any",
                    "typing.Mapping"
                ]
            }
        },
        "PositionEncoder.get_input_dim": {
            "name": "get_input_dim",
            "location": 51,
            "return": [
                "str",
                "bool"
            ],
            "arguments": {
                "self": []
            }
        },
        "PositionEncoder.get_output_dim": {
            "name": "get_output_dim",
            "location": 55,
            "return": [
                "str",
                "bool",
                "bytes"
            ],
            "arguments": {
                "self": []
            }
        }
    },
    "literate-lamp-master/literate_lamp/modules/xlnet_embedder.py": {
        "PretrainedXLNetModel.load": {
            "name": "load",
            "location": 26,
            "return": [
                "str",
                "bool"
            ],
            "arguments": {
                "cls": [
                    "dict[, ]",
                    "pathlib.Path",
                    "int",
                    "str",
                    "dict",
                    "Union[pathlib.Path, str]",
                    "None",
                    "Optional[str]"
                ],
                "config_path": [
                    "pathlib.Path"
                ],
                "model_path": [
                    "pathlib.Path"
                ],
                "cache_model": [
                    "bool"
                ]
            }
        },
        "XLNetEmbedder.__init__": {
            "name": "__init__",
            "location": 57,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "xlnet_model": [
                    "str",
                    "bool",
                    "dict",
                    "dict[, ]"
                ],
                "window_size": [
                    "Optional[int]"
                ]
            }
        },
        "XLNetEmbedder.get_output_dim": {
            "name": "get_output_dim",
            "location": 66,
            "return": [
                "int"
            ],
            "arguments": {
                "self": []
            }
        },
        "XLNetEmbedder.forward": {
            "name": "forward",
            "location": 69,
            "return": [
                "List[int]",
                "str"
            ],
            "arguments": {
                "self": [],
                "input_ids": [
                    "Optional[torch.BoolTensor]",
                    "None"
                ],
                "cls_indexes": [
                    "None",
                    "bool",
                    "Optional[torch.Tensor]",
                    "str",
                    "torch.BoolTensor",
                    "int"
                ],
                "token_type_ids": [
                    "None",
                    "Optional[torch.LongTensor]",
                    "str",
                    "int",
                    "torch.BoolTensor",
                    "torch.LongTensor",
                    "Optional[str]"
                ]
            }
        },
        "PretrainedXLNetEmbedder.__init__": {
            "name": "__init__",
            "location": 137,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "config_path": [
                    "pathlib.Path"
                ],
                "model_path": [
                    "pathlib.Path"
                ],
                "window_size": [
                    "Optional[int]"
                ],
                "requires_grad": [
                    "bool"
                ]
            }
        }
    },
    "literate-lamp-master/literate_lamp/modules/xlnet_indexer.py": {
        "XLNetIndexer.__init__": {
            "name": "__init__",
            "location": 39,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "namespace": [
                    "str"
                ],
                "vocab_file": [
                    "str"
                ],
                "sep_token": [
                    "str"
                ],
                "cls_token": [
                    "str"
                ],
                "pad_token": [
                    "Union[(int, str)]"
                ],
                "token_min_padding_length": [
                    "int"
                ]
            }
        },
        "XLNetIndexer.count_vocab_items": {
            "name": "count_vocab_items",
            "location": 59,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "token": [
                    "dict[str, dict[str, int]]",
                    "allennlp.data.tokenizers.token.Token",
                    "Dict[str, Dict[str, int]]"
                ],
                "counter": [
                    "dict[str, dict[str, int]]",
                    "allennlp.data.tokenizers.token.Token",
                    "Dict[str, Dict[str, int]]"
                ]
            }
        },
        "XLNetIndexer.tokens_to_indices": {
            "name": "tokens_to_indices",
            "location": 66,
            "return": [
                "dict[str, list[int]]",
                "str",
                "Dict[str, str]",
                "dict"
            ],
            "arguments": {
                "self": [],
                "tokens": [
                    "str",
                    "Optional[str]",
                    "dict[, ]",
                    "dict[str, str]",
                    "dict",
                    "Dict[str, str]"
                ],
                "vocabulary": [
                    "str",
                    "allennlp.data.vocabulary.Vocabulary",
                    "list[allennlp.data.tokenizers.token.Token]",
                    "List[allennlp.data.tokenizers.token.Token]"
                ],
                "index_name": [
                    "str"
                ]
            }
        },
        "XLNetIndexer.get_padding_token": {
            "name": "get_padding_token",
            "location": 104,
            "return": [
                "int",
                "str",
                "tuple",
                "List[str]"
            ],
            "arguments": {
                "self": []
            }
        },
        "XLNetIndexer.get_padding_lengths": {
            "name": "get_padding_lengths",
            "location": 108,
            "return": [
                "dict[, ]",
                "str",
                "bool"
            ],
            "arguments": {
                "self": [],
                "token": [
                    "int",
                    "str",
                    "List[int]",
                    "list[int]"
                ]
            }
        },
        "XLNetIndexer.pad_token_sequence": {
            "name": "pad_token_sequence",
            "location": 112,
            "return": [
                "dict[typing.Union[tuple[typing.Union[str,list[int]]],tuple[typing.Union[str,int]]], ]",
                "dict",
                "Callable",
                "Dict[str, float]"
            ],
            "arguments": {
                "self": [],
                "tokens": [
                    "Dict[str, List[int]]",
                    "dict[str, list[int]]",
                    "Dict[str, int]",
                    "dict[str, int]"
                ],
                "desired_num_tokens": [
                    "Dict[str, List[int]]",
                    "Dict[str, int]"
                ],
                "padding_lengths": [
                    "Dict[str, int]",
                    "int",
                    "dict[str, int]"
                ]
            }
        },
        "XLNetIndexer.get_keys": {
            "name": "get_keys",
            "location": 121,
            "return": [
                "list[typing.Optional[str]]",
                "str",
                "Optional[str]",
                "List[str]"
            ],
            "arguments": {
                "self": [],
                "index_name": [
                    "str",
                    "Optional[str]",
                    "None"
                ]
            }
        }
    },
    "literate-lamp-master/literate_lamp/modules/xlnet_pooler.py": {
        "XLNetPooler.__init__": {
            "name": "__init__",
            "location": 9,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "input_dim": [
                    "int"
                ]
            }
        },
        "XLNetPooler.get_input_dim": {
            "name": "get_input_dim",
            "location": 16,
            "return": [
                "str",
                "bool"
            ],
            "arguments": {
                "self": []
            }
        },
        "XLNetPooler.get_output_dim": {
            "name": "get_output_dim",
            "location": 20,
            "return": [
                "str",
                "Optional[str]"
            ],
            "arguments": {
                "self": []
            }
        },
        "XLNetPooler.forward": {
            "name": "forward",
            "location": 23,
            "return": [
                "str",
                "list[]",
                "dict[, ]"
            ],
            "arguments": {
                "self": [],
                "hidden_states": [
                    "torch.Tensor",
                    "float",
                    "Dict[str, torch.Tensor]"
                ],
                "cls_index": [
                    "None",
                    "Optional[Tuple[torch.Tensor, torch.Tensor]]",
                    "tuple[typing.Union[torch.Tensor,torch.Tensor]]",
                    "Optional[torch.Tensor]",
                    "dict[str, torch.LongTensor]",
                    "int",
                    "Dict[str, torch.LongTensor]",
                    "typing.Callable[str, int]",
                    "Union[int, None]",
                    "torch.LongTensor",
                    "torch.Tensor",
                    "Callable[[str], int]"
                ]
            }
        }
    },
    "literate-lamp-master/literate_lamp/modules/xlnet_word_splitter.py": {
        "PretrainedXLNetTokenizer.load": {
            "name": "load",
            "location": 18,
            "return": [
                "XLNetTokenizer",
                "str",
                "bool"
            ],
            "arguments": {
                "cls": [
                    "dict",
                    "bytes",
                    "dict[, ]",
                    "str"
                ],
                "vocab_file": [
                    "str"
                ],
                "cache_model": [
                    "bool"
                ]
            }
        },
        "XLNetWordSplitter.__init__": {
            "name": "__init__",
            "location": 36,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "vocab_file": [
                    "str"
                ],
                "do_lower_case": [
                    "bool"
                ]
            }
        },
        "XLNetWordSplitter.split_words": {
            "name": "split_words",
            "location": 42,
            "return": [
                "list"
            ],
            "arguments": {
                "self": [],
                "sentence": [
                    "str"
                ]
            }
        }
    },
    "literate-lamp-master/literate_lamp/modules/__init__.py": {},
    "literate-lamp-master/literate_lamp/readers/base_reader.py": {
        "BaseReader._read": {
            "name": "_read",
            "location": 20,
            "return": [
                "Iterator"
            ],
            "arguments": {
                "self": [],
                "file_path": [
                    "str"
                ]
            }
        }
    },
    "literate-lamp-master/literate_lamp/readers/extended_xlnet_reader.py": {
        "ExtendedXLNetReader.__init__": {
            "name": "__init__",
            "location": 44,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "vocab_file": [
                    "pathlib.Path"
                ],
                "conceptnet_path": [
                    "pathlib.Path"
                ],
                "word_indexer": [
                    "None",
                    "str",
                    "Optional[str]",
                    "Optional[pathlib.Path]"
                ]
            }
        },
        "ExtendedXLNetReader.text_to_instance": {
            "name": "text_to_instance",
            "location": 66,
            "return": [
                "Instance",
                "List[str]",
                "List[Tuple[str, str]]",
                "str",
                "Set[str]",
                "List[Tuple[str, Any]]"
            ],
            "arguments": {
                "self": [],
                "passage_id": [
                    "str"
                ],
                "question_id": [
                    "str"
                ],
                "question_type": [
                    "str"
                ],
                "passage": [
                    "str"
                ],
                "question": [
                    "str"
                ],
                "answer0": [
                    "str"
                ],
                "answer1": [
                    "str"
                ],
                "label0": [
                    "Optional[str]"
                ]
            }
        },
        "ExtendedXLNetReader.extend_passage": {
            "name": "extend_passage",
            "location": 109,
            "return": [
                "str"
            ],
            "arguments": {
                "self": [],
                "passage": [
                    "str"
                ],
                "question": [
                    "str"
                ],
                "answer": [
                    "str"
                ]
            }
        }
    },
    "literate-lamp-master/literate_lamp/readers/full_trian_reader.py": {
        "FullTrianReader.__init__": {
            "name": "__init__",
            "location": 54,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "word_indexer": [
                    "float",
                    "None",
                    "bool",
                    "int",
                    "list",
                    "list[]"
                ],
                "is_bert": [
                    "bool"
                ],
                "conceptnet_path": [
                    "Optional[pathlib.Path]"
                ]
            }
        },
        "FullTrianReader.text_to_instance": {
            "name": "text_to_instance",
            "location": 94,
            "return": [
                "Instance",
                "List[str]",
                "List[Tuple[str, str]]",
                "str",
                "Set[str]",
                "List[Tuple[str, Any]]"
            ],
            "arguments": {
                "self": [],
                "passage_id": [
                    "str"
                ],
                "question_id": [
                    "str"
                ],
                "question_type": [
                    "str"
                ],
                "passage": [
                    "str"
                ],
                "question": [
                    "str"
                ],
                "answer0": [
                    "str"
                ],
                "answer1": [
                    "str"
                ],
                "label0": [
                    "Optional[str]"
                ]
            }
        }
    },
    "literate-lamp-master/literate_lamp/readers/relation_bert_reader.py": {
        "RelationBertReader.__init__": {
            "name": "__init__",
            "location": 40,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "is_bert": [
                    "bool"
                ],
                "conceptnet_path": [
                    "pathlib.Path"
                ],
                "word_indexer": [
                    "None",
                    "str",
                    "Optional[allennlp.data.token_indexers.TokenIndexer]",
                    "bool",
                    "Optional[str]",
                    "int"
                ]
            }
        },
        "RelationBertReader.text_to_instance": {
            "name": "text_to_instance",
            "location": 72,
            "return": [
                "List[str]",
                "List[Tuple[str, str]]",
                "str",
                "Set[str]",
                "List[Tuple[str, Any]]"
            ],
            "arguments": {
                "self": [],
                "passage_id": [
                    "str"
                ],
                "question_id": [
                    "str"
                ],
                "question_type": [
                    "str"
                ],
                "passage": [
                    "str"
                ],
                "question": [
                    "str"
                ],
                "answer0": [
                    "str"
                ],
                "answer1": [
                    "str"
                ],
                "label0": [
                    "Optional[str]"
                ]
            }
        }
    },
    "literate-lamp-master/literate_lamp/readers/relation_xlnet_reader.py": {
        "RelationXLNetReader.__init__": {
            "name": "__init__",
            "location": 40,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "vocab_file": [
                    "pathlib.Path"
                ],
                "conceptnet_path": [
                    "pathlib.Path"
                ],
                "word_indexer": [
                    "None",
                    "str",
                    "Optional[str]",
                    "Optional[pathlib.Path]"
                ]
            }
        },
        "RelationXLNetReader.text_to_instance": {
            "name": "text_to_instance",
            "location": 62,
            "return": [
                "Instance",
                "List[str]",
                "List[Tuple[str, str]]",
                "str",
                "Set[str]",
                "List[Tuple[str, Any]]"
            ],
            "arguments": {
                "self": [],
                "passage_id": [
                    "str"
                ],
                "question_id": [
                    "str"
                ],
                "question_type": [
                    "str"
                ],
                "passage": [
                    "str"
                ],
                "question": [
                    "str"
                ],
                "answer0": [
                    "str"
                ],
                "answer1": [
                    "str"
                ],
                "label0": [
                    "Optional[str]"
                ]
            }
        }
    },
    "literate-lamp-master/literate_lamp/readers/sentence_reader.py": {
        "SentenceReader.__init__": {
            "name": "__init__",
            "location": 47,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "embedding_type": [
                    "str"
                ],
                "xlnet_vocab_file": [
                    "Optional[pathlib.Path]"
                ],
                "word_indexer": [
                    "Optional[str]",
                    "None",
                    "Optional[allennlp.data.token_indexers.TokenIndexer]",
                    "str"
                ]
            }
        },
        "SentenceReader.text_to_instance": {
            "name": "text_to_instance",
            "location": 70,
            "return": [
                "Instance",
                "List[str]",
                "List[Tuple[str, str]]",
                "str",
                "Set[str]",
                "List[Tuple[str, Any]]"
            ],
            "arguments": {
                "self": [],
                "passage_id": [
                    "str"
                ],
                "question_id": [
                    "str"
                ],
                "question_type": [
                    "str"
                ],
                "passage": [
                    "str"
                ],
                "question": [
                    "str"
                ],
                "answer0": [
                    "str"
                ],
                "answer1": [
                    "str"
                ],
                "label0": [
                    "Optional[str]"
                ]
            }
        }
    },
    "literate-lamp-master/literate_lamp/readers/simple_bert_reader.py": {
        "SimpleBertReader.__init__": {
            "name": "__init__",
            "location": 35,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "word_indexer": [
                    "Optional[allennlp.data.token_indexers.TokenIndexer]",
                    "None",
                    "str",
                    "Optional[int]",
                    "float",
                    "int"
                ]
            }
        },
        "SimpleBertReader.text_to_instance": {
            "name": "text_to_instance",
            "location": 56,
            "return": [
                "Instance",
                "List[str]",
                "List[Tuple[str, str]]",
                "str",
                "Set[str]",
                "List[Tuple[str, Any]]"
            ],
            "arguments": {
                "self": [],
                "passage_id": [
                    "str"
                ],
                "question_id": [
                    "str"
                ],
                "question_type": [
                    "str"
                ],
                "passage": [
                    "str"
                ],
                "question": [
                    "str"
                ],
                "answer0": [
                    "str"
                ],
                "answer1": [
                    "str"
                ],
                "label0": [
                    "Optional[str]"
                ]
            }
        }
    },
    "literate-lamp-master/literate_lamp/readers/simple_mc_script_reader.py": {
        "SimpleMcScriptReader.__init__": {
            "name": "__init__",
            "location": 35,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "embedding_type": [
                    "str"
                ],
                "xlnet_vocab_file": [
                    "Optional[pathlib.Path]"
                ],
                "word_indexer": [
                    "None",
                    "str",
                    "Optional[str]",
                    "typing.Sequence[str]",
                    "Optional[allennlp.data.token_indexers.TokenIndexer]",
                    "Sequence",
                    "Optional[Sequence[str]]"
                ]
            }
        },
        "SimpleMcScriptReader.text_to_instance": {
            "name": "text_to_instance",
            "location": 56,
            "return": [
                "Instance",
                "List[str]",
                "List[Tuple[str, str]]",
                "str",
                "Set[str]",
                "List[Tuple[str, Any]]"
            ],
            "arguments": {
                "self": [],
                "passage_id": [
                    "str"
                ],
                "question_id": [
                    "str"
                ],
                "question_type": [
                    "str"
                ],
                "passage": [
                    "str"
                ],
                "question": [
                    "str"
                ],
                "answer0": [
                    "str"
                ],
                "answer1": [
                    "str"
                ],
                "label0": [
                    "Optional[str]"
                ]
            }
        }
    },
    "literate-lamp-master/literate_lamp/readers/simple_trian_reader.py": {
        "SimpleTrianReader.__init__": {
            "name": "__init__",
            "location": 39,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "word_indexer": [
                    "None",
                    "str",
                    "Optional[allennlp.data.token_indexers.TokenIndexer]",
                    "Optional[str]"
                ],
                "xlnet_vocab_file": [
                    "Optional[pathlib.Path]"
                ],
                "embedding_type": [
                    "str"
                ],
                "conceptnet_path": [
                    "Optional[pathlib.Path]"
                ]
            }
        },
        "SimpleTrianReader.text_to_instance": {
            "name": "text_to_instance",
            "location": 62,
            "return": [
                "Instance",
                "List[str]",
                "List[Tuple[str, str]]",
                "str",
                "Set[str]",
                "List[Tuple[str, Any]]"
            ],
            "arguments": {
                "self": [],
                "passage_id": [
                    "str"
                ],
                "question_id": [
                    "str"
                ],
                "question_type": [
                    "str"
                ],
                "passage": [
                    "str"
                ],
                "question": [
                    "str"
                ],
                "answer0": [
                    "str"
                ],
                "answer1": [
                    "str"
                ],
                "label0": [
                    "Optional[str]"
                ]
            }
        }
    },
    "literate-lamp-master/literate_lamp/readers/simple_xlnet_reader.py": {
        "SimpleXLNetReader.__init__": {
            "name": "__init__",
            "location": 45,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "vocab_file": [
                    "pathlib.Path"
                ],
                "word_indexer": [
                    "None",
                    "str",
                    "pathlib.Path",
                    "list[str]",
                    "dict[str, str]",
                    "Union[str, List[str]]",
                    "int",
                    "Optional[Dict[str, str]]",
                    "Optional[int]",
                    "Optional[str]",
                    "Optional[pathlib.Path]"
                ]
            }
        },
        "SimpleXLNetReader.text_to_instance": {
            "name": "text_to_instance",
            "location": 65,
            "return": [
                "Instance",
                "List[str]",
                "List[Tuple[str, str]]",
                "str",
                "Set[str]",
                "List[Tuple[str, Any]]"
            ],
            "arguments": {
                "self": [],
                "passage_id": [
                    "str"
                ],
                "question_id": [
                    "str"
                ],
                "question_type": [
                    "str"
                ],
                "passage": [
                    "str"
                ],
                "question": [
                    "str"
                ],
                "answer0": [
                    "str"
                ],
                "answer1": [
                    "str"
                ],
                "label0": [
                    "Optional[str]"
                ]
            }
        }
    },
    "literate-lamp-master/literate_lamp/readers/util.py": {
        "strs2toks": {
            "name": "strs2toks",
            "location": 20,
            "return": [
                "list"
            ],
            "arguments": {
                "strings": [
                    "Sequence[str]"
                ]
            }
        },
        "toks2strs": {
            "name": "toks2strs",
            "location": 25,
            "return": [
                "List[str]"
            ],
            "arguments": {
                "tokens": [
                    "Sequence"
                ]
            }
        },
        "pieces2strs": {
            "name": "pieces2strs",
            "location": 30,
            "return": [
                "List[str]"
            ],
            "arguments": {
                "tokens": [
                    "Sequence"
                ]
            }
        },
        "compute_handcrafted_features": {
            "name": "compute_handcrafted_features",
            "location": 40,
            "return": [
                "np.ndarray"
            ],
            "arguments": {
                "passage": [
                    "Sequence"
                ],
                "question": [
                    "Sequence"
                ],
                "answer0": [
                    "Sequence"
                ],
                "answer1": [
                    "Sequence"
                ]
            }
        },
        "bert_sliding_window": {
            "name": "bert_sliding_window",
            "location": 83,
            "return": [
                "List[str]"
            ],
            "arguments": {
                "question": [
                    "str"
                ],
                "answer": [
                    "str"
                ],
                "passage": [
                    "str"
                ],
                "max_wordpieces": [
                    "int"
                ],
                "stride": [
                    "Optional[int]"
                ]
            }
        },
        "xlnet_input_string": {
            "name": "xlnet_input_string",
            "location": 101,
            "return": [
                "str"
            ],
            "arguments": {
                "question": [
                    "str"
                ],
                "answer": [
                    "str"
                ],
                "passage": [
                    "str"
                ]
            }
        },
        "relation_sentences": {
            "name": "relation_sentences",
            "location": 105,
            "return": [
                "List[str]"
            ],
            "arguments": {
                "conceptnet": [
                    "str",
                    "int",
                    "Union[str, int]",
                    "util.k8s.k8s_info.PodStatus"
                ],
                "text": [
                    "Sequence[str]"
                ],
                "query": [
                    "Sequence[str]"
                ]
            }
        },
        "get_tokenizer": {
            "name": "get_tokenizer",
            "location": 112,
            "return": [
                "WordTokenizer",
                "str",
                "bool"
            ],
            "arguments": {
                "embedding_type": [
                    "str"
                ],
                "xlnet_vocab_file": [
                    "pathlib.Path"
                ]
            }
        },
        "get_indexer": {
            "name": "get_indexer",
            "location": 124,
            "return": [
                "PretrainedBertIndexer",
                "str",
                "SingleIdTokenIndexer",
                "Optional[str]",
                "XLNetIndexer",
                "dict",
                "bool"
            ],
            "arguments": {
                "embedding_type": [
                    "str"
                ],
                "xlnet_vocab_file": [
                    "pathlib.Path"
                ]
            }
        },
        "split_sentences": {
            "name": "split_sentences",
            "location": 137,
            "return": [
                "Iterator[str]"
            ],
            "arguments": {
                "nlp": [
                    "str"
                ],
                "text": [
                    "str"
                ]
            }
        },
        "get_sentencizer": {
            "name": "get_sentencizer",
            "location": 143,
            "return": [
                "English",
                "str",
                "bool"
            ],
            "arguments": {}
        },
        "compute_handcrafted_features.is_valid": {
            "name": "is_valid",
            "location": 44,
            "return": [
                "bool"
            ],
            "arguments": {
                "token": [
                    "ebl.transliteration.domain.tokens.Token",
                    "bytes",
                    "List[str]",
                    "oj.tokens.Token",
                    "List[List[str]]",
                    "str"
                ]
            }
        },
        "compute_handcrafted_features.co_occurrence": {
            "name": "co_occurrence",
            "location": 47,
            "return": [
                "bool",
                "list[bool]"
            ],
            "arguments": {
                "text": [
                    "int",
                    "bytes",
                    "Tuple[int, int]",
                    "str",
                    "float"
                ],
                "query": [
                    "List[str]",
                    "str",
                    "Set[str]",
                    "Dict[int, Dict[str, Any]]",
                    "Iterable[str]"
                ]
            }
        }
    },
    "literate-lamp-master/literate_lamp/readers/__init__.py": {}
}
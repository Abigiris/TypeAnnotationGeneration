{
    "my_agents-master/setup.py": {},
    "my_agents-master/my_agents/comparison.py": {},
    "my_agents-master/my_agents/evaluate.py": {},
    "my_agents-master/my_agents/main.py": {},
    "my_agents-master/my_agents/vectorize_test.py": {},
    "my_agents-master/my_agents/agents/ddqn_agent.py": {
        "build_dense_network": {
            "name": "build_dense_network",
            "location": 24,
            "return": [
                "Pattern",
                "int",
                "str"
            ],
            "arguments": {
                "num_actions": [
                    "Dict",
                    "bool"
                ],
                "state_shape": [],
                "hidden_layers": [
                    "List[int]",
                    "int",
                    "bool",
                    "Optional[str]"
                ]
            }
        },
        "DDQNAgent.__init__": {
            "name": "__init__",
            "location": 63,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "num_actions": [
                    "Optional[str]",
                    "str",
                    "int",
                    "Optional[int]",
                    "Tuple",
                    "None"
                ],
                "state_shape": [
                    "Optional[str]",
                    "str",
                    "int",
                    "Optional[int]",
                    "Tuple",
                    "None"
                ],
                "gamma": [
                    "float",
                    "Optional[str]",
                    "Optional[bool]",
                    "numpy.ndarray"
                ],
                "target_update_freq": [
                    "int",
                    "bool",
                    "bytes",
                    "Optional[bool]",
                    "List[str]"
                ],
                "prebuilt_model": [
                    "bool",
                    "None",
                    "Optional[str]",
                    "Dict[str,np.ndarray]",
                    "Optional[bool]",
                    "str",
                    "Optional[Tuple[float,float]]",
                    "Tuple[Union[float,float]]"
                ]
            }
        },
        "DDQNAgent.act": {
            "name": "act",
            "location": 82,
            "return": [
                "Callable"
            ],
            "arguments": {
                "self": [],
                "state": [
                    "int",
                    "numpy.ndarray",
                    "Dict[str,Any]",
                    "Dict"
                ]
            }
        },
        "DDQNAgent.process_observation": {
            "name": "process_observation",
            "location": 86,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "state": [
                    "int",
                    "List[int]",
                    "List[list[int]]",
                    "List[List[int]]",
                    "List[Entity]",
                    "Callable[T,bool]",
                    "bool",
                    "Callable[[Type,str],bool]"
                ],
                "action": [
                    "int",
                    "List[int]",
                    "List[list[int]]",
                    "List[List[int]]",
                    "List[Entity]",
                    "Callable[T,bool]",
                    "bool",
                    "Callable[[Type,str],bool]"
                ],
                "reward": [
                    "int",
                    "List[int]",
                    "List[list[int]]",
                    "List[List[int]]",
                    "List[Entity]",
                    "Callable[T,bool]",
                    "bool",
                    "Callable[[Type,str],bool]"
                ],
                "next_state": [
                    "int",
                    "List[int]",
                    "List[list[int]]",
                    "List[List[int]]",
                    "List[Entity]",
                    "Callable[T,bool]",
                    "bool",
                    "Callable[[Type,str],bool]"
                ],
                "done": [
                    "int",
                    "List[int]",
                    "List[list[int]]",
                    "List[List[int]]",
                    "List[Entity]",
                    "Callable[T,bool]",
                    "bool",
                    "Callable[[Type,str],bool]"
                ]
            }
        },
        "DDQNAgent.train": {
            "name": "train",
            "location": 91,
            "return": [
                "int",
                "Dict[str,Any]",
                "None",
                "bytes",
                "Tuple[Union[int,int]]"
            ],
            "arguments": {
                "self": [],
                "step_num": [
                    "int",
                    "Callable",
                    "float"
                ],
                "batch_size": [
                    "int",
                    "float",
                    "bool",
                    "Optional[tensorflow.keras.initializers.Initializer]",
                    "Optional[int]",
                    "numpy.ndarray",
                    "Tuple[int,int]"
                ],
                "epochs": [
                    "int",
                    "Union[Tuple[int,int],int]",
                    "float",
                    "str"
                ]
            }
        },
        "DDQNAgent._update_target_model": {
            "name": "_update_target_model",
            "location": 112,
            "return": [
                "None"
            ],
            "arguments": {
                "self": []
            }
        },
        "DDQNAgent._observations_to_train_data": {
            "name": "_observations_to_train_data",
            "location": 115,
            "return": [
                "Tuple[(_T0, Any)]"
            ],
            "arguments": {
                "self": [],
                "states": [
                    "Sequence[int]",
                    "numpy.ndarray",
                    "List",
                    "List[list]",
                    "List[str]"
                ],
                "actions": [
                    "numpy.ndarray",
                    "List[int]",
                    "bool",
                    "List[List[int]]",
                    "Iterable[Any]",
                    "str"
                ],
                "rewards": [
                    "Tuple[Union[int,...]]",
                    "numpy.ndarray",
                    "bool",
                    "None",
                    "str",
                    "numpy.dtype",
                    "Optional[Tuple[int,...]]",
                    "int"
                ],
                "next_states": [],
                "dones": [
                    "Tuple[Union[int,...]]",
                    "numpy.ndarray",
                    "bool",
                    "None",
                    "str",
                    "numpy.dtype",
                    "Optional[Tuple[int,...]]",
                    "int"
                ]
            }
        },
        "DDQNAgent.Q": {
            "name": "Q",
            "location": 129,
            "return": [
                "dict",
                "bool"
            ],
            "arguments": {
                "self": [],
                "states": [
                    "str",
                    "List[str]"
                ],
                "use_target": [
                    "bool",
                    "int",
                    "str"
                ]
            }
        },
        "DDQNAgent.policy": {
            "name": "policy",
            "location": 142,
            "return": [
                "int",
                "str",
                "float",
                "Optional[str]"
            ],
            "arguments": {
                "self": [],
                "states": [
                    "bool",
                    "Dict",
                    "Optional[str]",
                    "str",
                    "None",
                    "int"
                ],
                "use_target": [
                    "bool",
                    "Optional[str]",
                    "Dict",
                    "int"
                ]
            }
        },
        "DDQNAgent.V": {
            "name": "V",
            "location": 146,
            "return": [
                "int",
                "str",
                "float",
                "Optional[str]",
                "List[Tuple[int, int]]"
            ],
            "arguments": {
                "self": [],
                "states": [
                    "bool"
                ],
                "use_target": [
                    "bool"
                ]
            }
        },
        "DDQNAgent.save": {
            "name": "save",
            "location": 150,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "file_path": [
                    "Text",
                    "str",
                    "BinaryIO"
                ]
            }
        },
        "DDQNAgent.from_h5": {
            "name": "from_h5",
            "location": 158,
            "return": [
                "DDQNAgent"
            ],
            "arguments": {
                "file_path": [
                    "Text",
                    "str",
                    "bool"
                ],
                "gamma": [
                    "float",
                    "bool",
                    "Optional[bool]",
                    "Optional[List[str]]",
                    "Optional[Union[Any,Any]]",
                    "Optional[str]"
                ],
                "target_update_freq": [
                    "int",
                    "bool",
                    "Optional[bool]",
                    "Optional[List[str]]",
                    "Optional[Union[Any,Any]]",
                    "Optional[str]"
                ]
            }
        }
    },
    "my_agents-master/my_agents/agents/distributional_agent.py": {
        "build_distributional_network": {
            "name": "build_distributional_network",
            "location": 19,
            "return": [
                "Pattern",
                "int",
                "str"
            ],
            "arguments": {
                "num_actions": [
                    "int",
                    "str",
                    "float",
                    "Optional[str]",
                    "None",
                    "bool",
                    "Optional[int]"
                ],
                "state_shape": [
                    "int",
                    "str",
                    "bytes",
                    "bool"
                ],
                "num_atoms": [
                    "int",
                    "str",
                    "float",
                    "Optional[str]",
                    "None",
                    "bool",
                    "Optional[int]"
                ],
                "hidden_layers": [
                    "List[int]",
                    "List",
                    "bool",
                    "bytes"
                ]
            }
        },
        "DistributionalAgent.__init__": {
            "name": "__init__",
            "location": 61,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "num_actions": [
                    "int",
                    "Tuple",
                    "None",
                    "Optional[int]",
                    "Union[int,None]"
                ],
                "state_shape": [
                    "int",
                    "Tuple",
                    "None",
                    "Optional[int]",
                    "Union[int,None]"
                ],
                "v_min": [
                    "int",
                    "float",
                    "str",
                    "None",
                    "Optional[float]",
                    "bool"
                ],
                "v_max": [
                    "int",
                    "float",
                    "str",
                    "None",
                    "Optional[float]",
                    "bool"
                ],
                "num_atoms": [
                    "int",
                    "Optional[int]",
                    "float",
                    "Iterable[str]",
                    "Callable"
                ],
                "gamma": [
                    "float",
                    "bool",
                    "numpy.array",
                    "str"
                ],
                "target_update_freq": [
                    "int",
                    "Optional[tensorflow.keras.regularizers.Regularizer]",
                    "bytes",
                    "bool",
                    "Optional[tensorflow.keras.initializers.Initializer]",
                    "Callable"
                ],
                "prebuilt_model": [
                    "Optional[List]",
                    "None",
                    "Optional[Any]",
                    "int",
                    "List",
                    "tensorflow.keras.models.Model",
                    "Any",
                    "Optional[str]",
                    "str",
                    "Optional[Dict[str,Any]]",
                    "Dict[str,Any]",
                    "Optional[Callable[[str],bool]]",
                    "Callable[str,bool]"
                ]
            }
        },
        "DistributionalAgent.act": {
            "name": "act",
            "location": 83,
            "return": [
                "Callable"
            ],
            "arguments": {
                "self": [],
                "state": [
                    "int",
                    "numpy.ndarray",
                    "Dict[str,Any]",
                    "Dict"
                ]
            }
        },
        "DistributionalAgent.process_observation": {
            "name": "process_observation",
            "location": 87,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "state": [
                    "int",
                    "List[int]",
                    "List[list[int]]",
                    "List[List[int]]",
                    "List[Entity]",
                    "Callable[T,bool]",
                    "bool",
                    "Callable[[Type,str],bool]"
                ],
                "action": [
                    "int",
                    "List[int]",
                    "List[list[int]]",
                    "List[List[int]]",
                    "List[Entity]",
                    "Callable[T,bool]",
                    "bool",
                    "Callable[[Type,str],bool]"
                ],
                "reward": [
                    "int",
                    "List[int]",
                    "List[list[int]]",
                    "List[List[int]]",
                    "List[Entity]",
                    "Callable[T,bool]",
                    "bool",
                    "Callable[[Type,str],bool]"
                ],
                "next_state": [
                    "int",
                    "List[int]",
                    "List[list[int]]",
                    "List[List[int]]",
                    "List[Entity]",
                    "Callable[T,bool]",
                    "bool",
                    "Callable[[Type,str],bool]"
                ],
                "done": [
                    "int",
                    "List[int]",
                    "List[list[int]]",
                    "List[List[int]]",
                    "List[Entity]",
                    "Callable[T,bool]",
                    "bool",
                    "Callable[[Type,str],bool]"
                ]
            }
        },
        "DistributionalAgent.train": {
            "name": "train",
            "location": 92,
            "return": [
                "int",
                "Dict[str,Any]",
                "None",
                "bytes",
                "Tuple[Union[int,int]]"
            ],
            "arguments": {
                "self": [],
                "step_num": [
                    "int",
                    "Callable",
                    "float"
                ],
                "batch_size": [
                    "int",
                    "float",
                    "bool",
                    "Optional[tensorflow.keras.initializers.Initializer]",
                    "Optional[int]",
                    "numpy.ndarray",
                    "Tuple[int,int]"
                ],
                "epochs": [
                    "int",
                    "Union[Tuple[int,int],int]",
                    "float",
                    "str"
                ]
            }
        },
        "DistributionalAgent._update_target_model": {
            "name": "_update_target_model",
            "location": 113,
            "return": [
                "None"
            ],
            "arguments": {
                "self": []
            }
        },
        "DistributionalAgent._observations_to_train_data": {
            "name": "_observations_to_train_data",
            "location": 116,
            "return": [
                "Tuple[(_T0, list)]"
            ],
            "arguments": {
                "self": [],
                "states": [
                    "Sequence[int]",
                    "numpy.ndarray",
                    "List",
                    "List[str]",
                    "List[list]"
                ],
                "actions": [
                    "numpy.ndarray",
                    "list"
                ],
                "rewards": [
                    "int",
                    "List[set[int]]",
                    "str",
                    "numpy.ndarray",
                    "List[Set[int]]",
                    "Iterable[str]",
                    "Iterable[Union[int,str]]",
                    "bool"
                ],
                "next_states": [
                    "List",
                    "numpy.ndarray"
                ],
                "dones": [
                    "int",
                    "str",
                    "bool",
                    "numpy.ndarray"
                ]
            }
        },
        "DistributionalAgent.Z": {
            "name": "Z",
            "location": 149,
            "return": [
                "np.ndarray"
            ],
            "arguments": {
                "self": [],
                "states": [
                    "str",
                    "int"
                ]
            }
        },
        "DistributionalAgent.Q": {
            "name": "Q",
            "location": 156,
            "return": [
                "int",
                "Callable",
                "str"
            ],
            "arguments": {
                "self": [],
                "states": [
                    "numpy.ndarray",
                    "List[int]",
                    "List[tensorflow.Tensor]",
                    "Optional[int]",
                    "bytes"
                ]
            }
        },
        "DistributionalAgent.policy": {
            "name": "policy",
            "location": 168,
            "return": [
                "dict",
                "bool"
            ],
            "arguments": {
                "self": [],
                "states": [
                    "int",
                    "numpy.ndarray",
                    "Dict[str,Any]",
                    "Dict"
                ]
            }
        },
        "DistributionalAgent.V": {
            "name": "V",
            "location": 172,
            "return": [
                "dict"
            ],
            "arguments": {
                "self": [],
                "states": [
                    "Dict",
                    "float",
                    "int",
                    "numpy.array"
                ]
            }
        },
        "DistributionalAgent.save": {
            "name": "save",
            "location": 176,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "file_path": [
                    "Text",
                    "str",
                    "BinaryIO"
                ]
            }
        },
        "DistributionalAgent.from_h5": {
            "name": "from_h5",
            "location": 184,
            "return": [
                "DistributionalAgent"
            ],
            "arguments": {
                "file_path": [
                    "Text",
                    "str",
                    "bool",
                    "int"
                ],
                "v_min": [
                    "int",
                    "Optional[int]",
                    "Optional[str]",
                    "List[str]",
                    "bool",
                    "Optional[List[str]]"
                ],
                "v_max": [
                    "int",
                    "Optional[int]",
                    "Optional[str]",
                    "List[str]",
                    "bool",
                    "Optional[List[str]]"
                ],
                "gamma": [
                    "float",
                    "Optional[int]",
                    "Optional[str]",
                    "List[str]",
                    "bool",
                    "Optional[List[str]]"
                ],
                "target_update_freq": [
                    "int",
                    "Optional[int]",
                    "Optional[str]",
                    "List[str]",
                    "bool",
                    "Optional[List[str]]"
                ]
            }
        },
        "DistributionalAgent.Distribution.__init__": {
            "name": "__init__",
            "location": 47,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "v_min": [
                    "int",
                    "float",
                    "List[int]"
                ],
                "v_max": [
                    "int",
                    "float"
                ],
                "num_atoms": [
                    "int",
                    "Optional[int]",
                    "bool"
                ]
            }
        },
        "DistributionalAgent.Distribution.project_to_distribution": {
            "name": "project_to_distribution",
            "location": 54,
            "return": [
                "Tuple[(Any, Any, Any)]"
            ],
            "arguments": {
                "self": [],
                "values": [
                    "str",
                    "Optional[str]"
                ]
            }
        }
    },
    "my_agents-master/my_agents/agents/dqn_agent.py": {
        "build_dense_network": {
            "name": "build_dense_network",
            "location": 24,
            "return": [
                "Pattern",
                "int",
                "str"
            ],
            "arguments": {
                "num_actions": [
                    "Dict",
                    "bool"
                ],
                "state_shape": [],
                "hidden_layers": [
                    "List[int]",
                    "int",
                    "bool",
                    "Optional[str]"
                ]
            }
        },
        "DQNAgent.__init__": {
            "name": "__init__",
            "location": 65,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "num_actions": [
                    "Optional[str]",
                    "str",
                    "int",
                    "Optional[int]",
                    "Tuple",
                    "None"
                ],
                "state_shape": [
                    "Optional[str]",
                    "str",
                    "int",
                    "Optional[int]",
                    "Tuple",
                    "None"
                ],
                "gamma": [
                    "float",
                    "Optional[str]",
                    "Optional[bool]",
                    "numpy.ndarray"
                ],
                "target_update_freq": [
                    "int",
                    "bool",
                    "bytes",
                    "Optional[bool]",
                    "List[str]"
                ],
                "prebuilt_model": [
                    "bool",
                    "None",
                    "Optional[str]",
                    "Dict[str,np.ndarray]",
                    "Optional[bool]",
                    "str",
                    "Optional[Tuple[float,float]]",
                    "Tuple[Union[float,float]]"
                ]
            }
        },
        "DQNAgent.act": {
            "name": "act",
            "location": 84,
            "return": [
                "Callable"
            ],
            "arguments": {
                "self": [],
                "state": [
                    "int",
                    "numpy.ndarray",
                    "Dict[str,Any]",
                    "Dict"
                ]
            }
        },
        "DQNAgent.process_observation": {
            "name": "process_observation",
            "location": 88,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "state": [
                    "int",
                    "List[int]",
                    "List[list[int]]",
                    "List[List[int]]",
                    "List[Entity]",
                    "Callable[T,bool]",
                    "bool",
                    "Callable[[Type,str],bool]"
                ],
                "action": [
                    "int",
                    "List[int]",
                    "List[list[int]]",
                    "List[List[int]]",
                    "List[Entity]",
                    "Callable[T,bool]",
                    "bool",
                    "Callable[[Type,str],bool]"
                ],
                "reward": [
                    "int",
                    "List[int]",
                    "List[list[int]]",
                    "List[List[int]]",
                    "List[Entity]",
                    "Callable[T,bool]",
                    "bool",
                    "Callable[[Type,str],bool]"
                ],
                "next_state": [
                    "int",
                    "List[int]",
                    "List[list[int]]",
                    "List[List[int]]",
                    "List[Entity]",
                    "Callable[T,bool]",
                    "bool",
                    "Callable[[Type,str],bool]"
                ],
                "done": [
                    "int",
                    "List[int]",
                    "List[list[int]]",
                    "List[List[int]]",
                    "List[Entity]",
                    "Callable[T,bool]",
                    "bool",
                    "Callable[[Type,str],bool]"
                ]
            }
        },
        "DQNAgent.train": {
            "name": "train",
            "location": 93,
            "return": [
                "int",
                "Dict[str,Any]",
                "None",
                "bytes",
                "Tuple[Union[int,int]]"
            ],
            "arguments": {
                "self": [],
                "step_num": [
                    "int",
                    "Callable",
                    "float"
                ],
                "batch_size": [
                    "int",
                    "float",
                    "bool",
                    "Optional[tensorflow.keras.initializers.Initializer]",
                    "Optional[int]",
                    "numpy.ndarray",
                    "Tuple[int,int]"
                ],
                "epochs": [
                    "int",
                    "Union[Tuple[int,int],int]",
                    "float",
                    "str"
                ]
            }
        },
        "DQNAgent._update_target_model": {
            "name": "_update_target_model",
            "location": 114,
            "return": [
                "None"
            ],
            "arguments": {
                "self": []
            }
        },
        "DQNAgent._observations_to_train_data": {
            "name": "_observations_to_train_data",
            "location": 117,
            "return": [
                "Tuple[(_T0, Any)]"
            ],
            "arguments": {
                "self": [],
                "states": [
                    "Sequence[int]",
                    "numpy.ndarray",
                    "List",
                    "List[list]",
                    "List[str]"
                ],
                "actions": [
                    "numpy.ndarray",
                    "List[int]",
                    "bool",
                    "List[List[int]]",
                    "Iterable[Any]",
                    "str"
                ],
                "rewards": [
                    "Tuple[Union[int,...]]",
                    "numpy.ndarray",
                    "int",
                    "None",
                    "Optional[Tuple[int,...]]",
                    "numpy.dtype"
                ],
                "next_states": [
                    "numpy.ndarray"
                ],
                "dones": [
                    "Tuple[Union[int,...]]",
                    "numpy.ndarray",
                    "int",
                    "None",
                    "Optional[Tuple[int,...]]",
                    "numpy.dtype"
                ]
            }
        },
        "DQNAgent.Q": {
            "name": "Q",
            "location": 131,
            "return": [
                "str",
                "Optional[bool]",
                "Optional[int]",
                "int"
            ],
            "arguments": {
                "self": [],
                "states": [
                    "Iterable[Any]",
                    "bytes"
                ]
            }
        },
        "DQNAgent.policy": {
            "name": "policy",
            "location": 140,
            "return": [
                "dict",
                "bool"
            ],
            "arguments": {
                "self": [],
                "states": [
                    "int",
                    "numpy.ndarray",
                    "Dict[str,Any]",
                    "Dict"
                ]
            }
        },
        "DQNAgent.V": {
            "name": "V",
            "location": 144,
            "return": [
                "dict"
            ],
            "arguments": {
                "self": [],
                "states": [
                    "Dict",
                    "float",
                    "int",
                    "numpy.array"
                ]
            }
        },
        "DQNAgent.save": {
            "name": "save",
            "location": 148,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "file_path": [
                    "Text",
                    "str",
                    "BinaryIO"
                ]
            }
        },
        "DQNAgent.from_h5": {
            "name": "from_h5",
            "location": 156,
            "return": [
                "DQNAgent"
            ],
            "arguments": {
                "file_path": [
                    "Text",
                    "str",
                    "bool"
                ],
                "gamma": [
                    "float",
                    "bool",
                    "Optional[bool]",
                    "Optional[List[str]]",
                    "Optional[Union[Any,Any]]",
                    "Optional[str]"
                ],
                "target_update_freq": [
                    "int",
                    "bool",
                    "Optional[bool]",
                    "Optional[List[str]]",
                    "Optional[Union[Any,Any]]",
                    "Optional[str]"
                ]
            }
        }
    },
    "my_agents-master/my_agents/agents/nstep_agent.py": {
        "build_dense_network": {
            "name": "build_dense_network",
            "location": 21,
            "return": [
                "Pattern",
                "int",
                "str"
            ],
            "arguments": {
                "num_actions": [
                    "Dict",
                    "bool"
                ],
                "state_shape": [],
                "hidden_layers": [
                    "List[int]",
                    "int",
                    "bool",
                    "Optional[str]"
                ]
            }
        },
        "NStepDDQNAgent.__init__": {
            "name": "__init__",
            "location": 61,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "num_actions": [
                    "int",
                    "Optional[int]",
                    "Optional[float]",
                    "Tuple",
                    "None",
                    "bytes",
                    "float",
                    "Optional[str]",
                    "str"
                ],
                "state_shape": [
                    "int",
                    "Optional[int]",
                    "Optional[float]",
                    "Tuple",
                    "None",
                    "bytes",
                    "float",
                    "Optional[str]",
                    "str"
                ],
                "update_horizon": [
                    "int",
                    "bool",
                    "Type[MutableMapping[Any,Any]]",
                    "Optional[int]",
                    "Tuple[int,int]"
                ],
                "gamma": [
                    "float",
                    "bool",
                    "int",
                    "Optional[dict]"
                ],
                "target_update_freq": [
                    "int",
                    "bool",
                    "bytes",
                    "Optional[bool]"
                ],
                "prebuilt_model": [
                    "None",
                    "Optional[bool]",
                    "bool",
                    "Dict[str,np.ndarray]",
                    "Optional[float]",
                    "float",
                    "Optional[Tuple[float,float]]",
                    "Tuple[Union[float,float]]",
                    "Union[None,int]",
                    "List",
                    "Optional[List]"
                ]
            }
        },
        "NStepDDQNAgent.act": {
            "name": "act",
            "location": 82,
            "return": [
                "Callable"
            ],
            "arguments": {
                "self": [],
                "state": [
                    "int",
                    "numpy.ndarray",
                    "Dict[str,Any]",
                    "Dict"
                ]
            }
        },
        "NStepDDQNAgent.process_observation": {
            "name": "process_observation",
            "location": 86,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "state": [
                    "int",
                    "List[int]",
                    "List[list[int]]",
                    "List[List[int]]",
                    "List[Entity]",
                    "Callable[T,bool]",
                    "bool",
                    "Callable[[Type,str],bool]"
                ],
                "action": [
                    "int",
                    "List[int]",
                    "List[list[int]]",
                    "List[List[int]]",
                    "List[Entity]",
                    "Callable[T,bool]",
                    "bool",
                    "Callable[[Type,str],bool]"
                ],
                "reward": [
                    "int",
                    "List[int]",
                    "List[list[int]]",
                    "List[List[int]]",
                    "List[Entity]",
                    "Callable[T,bool]",
                    "bool",
                    "Callable[[Type,str],bool]"
                ],
                "next_state": [
                    "int",
                    "List[int]",
                    "List[list[int]]",
                    "List[List[int]]",
                    "List[Entity]",
                    "Callable[T,bool]",
                    "bool",
                    "Callable[[Type,str],bool]"
                ],
                "done": [
                    "int",
                    "List[int]",
                    "List[list[int]]",
                    "List[List[int]]",
                    "List[Entity]",
                    "Callable[T,bool]",
                    "bool",
                    "Callable[[Type,str],bool]"
                ]
            }
        },
        "NStepDDQNAgent.train": {
            "name": "train",
            "location": 91,
            "return": [
                "int",
                "Dict[str,Any]",
                "None",
                "bytes",
                "Tuple[Union[int,int]]"
            ],
            "arguments": {
                "self": [],
                "step_num": [
                    "int",
                    "Callable",
                    "float"
                ],
                "batch_size": [
                    "int",
                    "List[float]",
                    "float"
                ],
                "epochs": [
                    "int",
                    "Union[Tuple[int,int],int]",
                    "float",
                    "str"
                ]
            }
        },
        "NStepDDQNAgent._sample_n_transitions": {
            "name": "_sample_n_transitions",
            "location": 113,
            "return": [
                "Tuple[(list, list, list, list, list, list)]"
            ],
            "arguments": {
                "self": [],
                "batch_size": [
                    "int",
                    "List",
                    "Tuple[Union[int,int]]",
                    "str",
                    "Tuple[int,int]",
                    "Sequence",
                    "bool"
                ]
            }
        },
        "NStepDDQNAgent._update_target_model": {
            "name": "_update_target_model",
            "location": 141,
            "return": [
                "None"
            ],
            "arguments": {
                "self": []
            }
        },
        "NStepDDQNAgent._observations_to_train_data": {
            "name": "_observations_to_train_data",
            "location": 144,
            "return": [
                "Tuple[(_T0, Any)]"
            ],
            "arguments": {
                "self": [],
                "states": [
                    "Sequence[int]",
                    "numpy.ndarray",
                    "List",
                    "List[list]",
                    "List[str]"
                ],
                "actions": [
                    "int",
                    "bool",
                    "numpy.ndarray"
                ],
                "rewards": [
                    "Callable",
                    "List[list[int]]",
                    "numpy.ndarray",
                    "List[List[int]]"
                ],
                "next_states": [],
                "dones": [
                    "Callable",
                    "List[list[int]]",
                    "numpy.ndarray",
                    "List[List[int]]"
                ],
                "gammas": [
                    "Callable",
                    "List[list[int]]",
                    "numpy.ndarray",
                    "List[List[int]]"
                ]
            }
        },
        "NStepDDQNAgent.Q": {
            "name": "Q",
            "location": 159,
            "return": [
                "dict",
                "bool"
            ],
            "arguments": {
                "self": [],
                "states": [
                    "str",
                    "List[str]"
                ],
                "use_target": [
                    "bool",
                    "int",
                    "str"
                ]
            }
        },
        "NStepDDQNAgent.policy": {
            "name": "policy",
            "location": 172,
            "return": [
                "int",
                "str",
                "float",
                "Optional[str]"
            ],
            "arguments": {
                "self": [],
                "states": [
                    "bool",
                    "Dict",
                    "Optional[str]",
                    "str",
                    "None",
                    "int"
                ],
                "use_target": [
                    "bool",
                    "Optional[str]",
                    "Dict",
                    "int"
                ]
            }
        },
        "NStepDDQNAgent.V": {
            "name": "V",
            "location": 176,
            "return": [
                "int",
                "str",
                "float",
                "Optional[str]",
                "List[Tuple[int, int]]"
            ],
            "arguments": {
                "self": [],
                "states": [
                    "bool"
                ],
                "use_target": [
                    "bool"
                ]
            }
        },
        "NStepDDQNAgent.save": {
            "name": "save",
            "location": 180,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "file_path": [
                    "Text",
                    "str",
                    "BinaryIO"
                ]
            }
        },
        "NStepDDQNAgent.from_h5": {
            "name": "from_h5",
            "location": 188,
            "return": [
                "NStepDDQNAgent"
            ],
            "arguments": {
                "file_path": [
                    "Text",
                    "str",
                    "bool"
                ],
                "update_horizon": [
                    "int",
                    "Optional[float]",
                    "List[dict]",
                    "Optional[bool]",
                    "Optional[str]",
                    "Optional[Dict[str,str]]"
                ],
                "gamma": [
                    "float",
                    "Optional[float]",
                    "List[dict]",
                    "Optional[bool]",
                    "Optional[str]",
                    "Optional[Dict[str,str]]",
                    "int"
                ],
                "target_update_freq": [
                    "int",
                    "Optional[float]",
                    "List[dict]",
                    "Optional[bool]",
                    "Optional[str]",
                    "Optional[Dict[str,str]]"
                ]
            }
        }
    },
    "my_agents-master/my_agents/agents/prioritized_ddqn_agent.py": {
        "build_dense_network": {
            "name": "build_dense_network",
            "location": 16,
            "return": [
                "Pattern",
                "int",
                "str"
            ],
            "arguments": {
                "num_actions": [
                    "Dict",
                    "bool"
                ],
                "state_shape": [],
                "hidden_layers": [
                    "List[int]",
                    "int",
                    "bool",
                    "Optional[str]"
                ]
            }
        },
        "PrioritizedDDQNAgent.__init__": {
            "name": "__init__",
            "location": 55,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "num_actions": [
                    "Optional[str]",
                    "str",
                    "int",
                    "Optional[int]",
                    "Tuple",
                    "None"
                ],
                "state_shape": [
                    "Optional[str]",
                    "str",
                    "int",
                    "Optional[int]",
                    "Tuple",
                    "None"
                ],
                "gamma": [
                    "float",
                    "Optional[str]",
                    "Optional[bool]",
                    "numpy.ndarray"
                ],
                "target_update_freq": [
                    "int",
                    "bool",
                    "bytes",
                    "Optional[bool]",
                    "List[str]"
                ],
                "prebuilt_model": [
                    "bool",
                    "None",
                    "Optional[str]",
                    "Dict[str,np.ndarray]",
                    "Optional[bool]",
                    "str",
                    "Optional[Tuple[float,float]]",
                    "Tuple[Union[float,float]]"
                ]
            }
        },
        "PrioritizedDDQNAgent.act": {
            "name": "act",
            "location": 74,
            "return": [
                "Callable"
            ],
            "arguments": {
                "self": [],
                "state": [
                    "int",
                    "numpy.ndarray",
                    "Dict[str,Any]",
                    "Dict"
                ]
            }
        },
        "PrioritizedDDQNAgent.process_observation": {
            "name": "process_observation",
            "location": 78,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "state": [
                    "bool",
                    "float",
                    "numpy.ndarray",
                    "int"
                ],
                "action": [
                    "bool",
                    "float",
                    "numpy.ndarray",
                    "int"
                ],
                "reward": [
                    "bool",
                    "float",
                    "numpy.ndarray",
                    "int"
                ],
                "next_state": [
                    "bool",
                    "float",
                    "numpy.ndarray",
                    "int"
                ],
                "done": [
                    "bool",
                    "float",
                    "numpy.ndarray",
                    "int"
                ]
            }
        },
        "PrioritizedDDQNAgent.train": {
            "name": "train",
            "location": 83,
            "return": [
                "int",
                "None",
                "Dict[str,Any]",
                "str"
            ],
            "arguments": {
                "self": [],
                "step_num": [
                    "int",
                    "Callable",
                    "float"
                ],
                "batch_size": [
                    "int",
                    "float",
                    "bool"
                ],
                "epochs": [
                    "int",
                    "List[float]",
                    "float",
                    "numpy.ndarray"
                ]
            }
        },
        "PrioritizedDDQNAgent._update_target_model": {
            "name": "_update_target_model",
            "location": 108,
            "return": [
                "None"
            ],
            "arguments": {
                "self": []
            }
        },
        "PrioritizedDDQNAgent._observations_to_train_data": {
            "name": "_observations_to_train_data",
            "location": 111,
            "return": [
                "Tuple[(_T0, Any)]"
            ],
            "arguments": {
                "self": [],
                "states": [
                    "Sequence[int]",
                    "numpy.ndarray",
                    "List",
                    "List[list]",
                    "List[str]"
                ],
                "actions": [
                    "numpy.ndarray",
                    "List[int]",
                    "bool",
                    "List[List[int]]",
                    "Iterable[Any]",
                    "str"
                ],
                "rewards": [
                    "Tuple[Union[int,...]]",
                    "numpy.ndarray",
                    "bool",
                    "None",
                    "str",
                    "numpy.dtype",
                    "Optional[Tuple[int,...]]",
                    "int"
                ],
                "next_states": [],
                "dones": [
                    "Tuple[Union[int,...]]",
                    "numpy.ndarray",
                    "bool",
                    "None",
                    "str",
                    "numpy.dtype",
                    "Optional[Tuple[int,...]]",
                    "int"
                ]
            }
        },
        "PrioritizedDDQNAgent.Q": {
            "name": "Q",
            "location": 125,
            "return": [
                "dict",
                "bool"
            ],
            "arguments": {
                "self": [],
                "states": [
                    "str",
                    "List[str]"
                ],
                "use_target": [
                    "bool",
                    "int",
                    "str"
                ]
            }
        },
        "PrioritizedDDQNAgent.policy": {
            "name": "policy",
            "location": 138,
            "return": [
                "int",
                "str",
                "float",
                "Optional[str]"
            ],
            "arguments": {
                "self": [],
                "states": [
                    "bool",
                    "Dict",
                    "Optional[str]",
                    "str",
                    "None",
                    "int"
                ],
                "use_target": [
                    "bool",
                    "Optional[str]",
                    "Dict",
                    "int"
                ]
            }
        },
        "PrioritizedDDQNAgent.V": {
            "name": "V",
            "location": 142,
            "return": [
                "int",
                "str",
                "float",
                "Optional[str]",
                "List[Tuple[int, int]]"
            ],
            "arguments": {
                "self": [],
                "states": [
                    "bool"
                ],
                "use_target": [
                    "bool"
                ]
            }
        },
        "PrioritizedDDQNAgent.save": {
            "name": "save",
            "location": 146,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "file_path": [
                    "Text",
                    "str",
                    "BinaryIO"
                ]
            }
        },
        "PrioritizedDDQNAgent.from_h5": {
            "name": "from_h5",
            "location": 154,
            "return": [
                "PrioritizedDDQNAgent"
            ],
            "arguments": {
                "file_path": [
                    "Text",
                    "str",
                    "bool"
                ],
                "gamma": [
                    "float",
                    "List[dict]",
                    "Optional[str]",
                    "bool",
                    "str",
                    "Callable[[Optional],None]"
                ],
                "target_update_freq": [
                    "int",
                    "List[dict]",
                    "Optional[str]",
                    "bool",
                    "str",
                    "Callable[[Optional],None]"
                ]
            }
        }
    },
    "my_agents-master/my_agents/agents/prioritized_memory.py": {
        "SumTree.__init__": {
            "name": "__init__",
            "location": 15,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "capacity": [
                    "bytes",
                    "str"
                ]
            }
        },
        "SumTree.add": {
            "name": "add",
            "location": 22,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "priority": [
                    "str",
                    "bool",
                    "int",
                    "Dict",
                    "Optional[Dict]",
                    "None"
                ],
                "data": [
                    "Dict",
                    "List[str]",
                    "Dict[str,str]"
                ]
            }
        },
        "SumTree.update": {
            "name": "update",
            "location": 36,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "tree_index": [
                    "dict"
                ],
                "priority": [
                    "Dict"
                ]
            }
        },
        "SumTree.get_leaf": {
            "name": "get_leaf",
            "location": 46,
            "return": [
                "Tuple[(int, Any, Any)]"
            ],
            "arguments": {
                "self": [],
                "v": [
                    "int",
                    "str",
                    "float",
                    "Optional[int]",
                    "None"
                ]
            }
        },
        "SumTree.total_priority": {
            "name": "total_priority",
            "location": 73,
            "return": [
                "str"
            ],
            "arguments": {
                "self": []
            }
        },
        "SumTree.max_priority": {
            "name": "max_priority",
            "location": 77,
            "return": [
                "float",
                "dict",
                "str",
                "List[str]",
                "int"
            ],
            "arguments": {
                "self": []
            }
        },
        "SumTree.min_priority": {
            "name": "min_priority",
            "location": 81,
            "return": [
                "float",
                "dict",
                "str",
                "List[str]",
                "int"
            ],
            "arguments": {
                "self": []
            }
        },
        "SumTree.__len__": {
            "name": "__len__",
            "location": 84,
            "return": [
                "int"
            ],
            "arguments": {
                "self": []
            }
        },
        "PrioritizedMemory.__init__": {
            "name": "__init__",
            "location": 95,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "capacity": [
                    "bool",
                    "Exception",
                    "List[str]",
                    "int"
                ],
                "alpha": [
                    "float",
                    "Callable",
                    "Union[float,Tuple[float,float]]",
                    "Optional[Callable[[Any],None]]",
                    "str"
                ],
                "beta": [
                    "float",
                    "Callable",
                    "Union[float,Tuple[float,float]]",
                    "Optional[Callable[[Any],None]]",
                    "str"
                ],
                "max_error": [
                    "int",
                    "float"
                ]
            }
        },
        "PrioritizedMemory.store": {
            "name": "store",
            "location": 103,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "experience": [
                    "int",
                    "VT"
                ]
            }
        },
        "PrioritizedMemory.sample": {
            "name": "sample",
            "location": 110,
            "return": [
                "Tuple[(Any, list, Any)]"
            ],
            "arguments": {
                "self": [],
                "batch_size": [
                    "int",
                    "float"
                ]
            }
        },
        "PrioritizedMemory.batch_update": {
            "name": "batch_update",
            "location": 141,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "tree_idx": [
                    "bool",
                    "float",
                    "List",
                    "List[tuple[Union[float,Any]]]",
                    "List[Tuple[float,Any]]",
                    "int",
                    "Iterable"
                ],
                "abs_errors": [
                    "str",
                    "bool"
                ]
            }
        },
        "PrioritizedMemory.__len__": {
            "name": "__len__",
            "location": 150,
            "return": [
                "int"
            ],
            "arguments": {
                "self": []
            }
        }
    },
    "my_agents-master/my_agents/agents/table_agent.py": {
        "TableAgent.__init__": {
            "name": "__init__",
            "location": 6,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "num_actions": [
                    "int",
                    "List[int]"
                ],
                "num_states": [
                    "int",
                    "str"
                ],
                "gamma": [
                    "float",
                    "int",
                    "str",
                    "Tuple"
                ],
                "alpha": [
                    "float",
                    "int",
                    "str",
                    "Tuple"
                ]
            }
        },
        "TableAgent.act": {
            "name": "act",
            "location": 14,
            "return": [
                "Callable"
            ],
            "arguments": {
                "self": [],
                "state": [
                    "int",
                    "numpy.ndarray",
                    "Dict[str,Any]",
                    "Dict"
                ]
            }
        },
        "TableAgent.process_observation": {
            "name": "process_observation",
            "location": 18,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "state": [
                    "bool"
                ],
                "action": [
                    "bool"
                ],
                "reward": [
                    "List[List[str]]",
                    "List[int]",
                    "List[list[str]]",
                    "bool",
                    "int",
                    "str",
                    "Optional[str]",
                    "None"
                ],
                "next_state": [
                    "str",
                    "int",
                    "List[int]",
                    "Tuple[int,int]",
                    "Tuple[Union[int,int]]",
                    "Tuple[float,float]",
                    "Tuple[Union[float,float]]"
                ],
                "done": [
                    "bool",
                    "List[Callable]"
                ]
            }
        },
        "TableAgent.train": {
            "name": "train",
            "location": 26,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "step_num": [
                    "bool",
                    "Iterable[T]",
                    "Sequence[str]",
                    "float",
                    "int"
                ]
            }
        },
        "TableAgent.Q": {
            "name": "Q",
            "location": 30,
            "return": [
                "bool"
            ],
            "arguments": {
                "self": [],
                "state": [
                    "int",
                    "numpy.ndarray",
                    "Dict[str,Any]",
                    "Dict"
                ]
            }
        },
        "TableAgent.policy": {
            "name": "policy",
            "location": 34,
            "return": [
                "dict",
                "bool"
            ],
            "arguments": {
                "self": [],
                "state": [
                    "int",
                    "numpy.ndarray",
                    "Dict[str,Any]",
                    "Dict"
                ]
            }
        },
        "TableAgent.V": {
            "name": "V",
            "location": 38,
            "return": [
                "dict"
            ],
            "arguments": {
                "self": [],
                "state": [
                    "Dict",
                    "float",
                    "int",
                    "numpy.array"
                ]
            }
        },
        "TableAgent.print_q_map": {
            "name": "print_q_map",
            "location": 42,
            "return": [
                "None"
            ],
            "arguments": {
                "self": []
            }
        }
    },
    "my_agents-master/my_agents/core/runner.py": {
        "constant_decay_epsilon": {
            "name": "constant_decay_epsilon",
            "location": 20,
            "return": [
                "int",
                "float",
                "Callable",
                "str"
            ],
            "arguments": {
                "epoch": [
                    "int",
                    "Callable",
                    "str"
                ],
                "initial_epsilon": [
                    "int",
                    "Callable",
                    "str"
                ],
                "decay_rate": [
                    "float",
                    "int",
                    "Callable",
                    "str"
                ],
                "min_epsilon": [
                    "float",
                    "int",
                    "Callable",
                    "str"
                ]
            }
        },
        "Runner.__init__": {
            "name": "__init__",
            "location": 30,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "env": [
                    "str",
                    "List[str]",
                    "Mapping",
                    "gym.Env",
                    "Mapping[str,str]"
                ],
                "serializer": [
                    "str",
                    "List[str]",
                    "Mapping",
                    "gym.Env",
                    "Mapping[str,str]"
                ],
                "agent": [
                    "str",
                    "List[str]",
                    "Mapping",
                    "gym.Env",
                    "Mapping[str,str]"
                ],
                "epsilon_policy": [
                    "Callable",
                    "str",
                    "gym.Env",
                    "Mapping[str,str]",
                    "List[str]"
                ],
                "training_period": [
                    "int",
                    "str",
                    "gym.Env",
                    "Mapping[str,str]",
                    "List[str]"
                ],
                "max_episode_steps": [
                    "int",
                    "float",
                    "bool"
                ]
            }
        },
        "Runner.warm_up": {
            "name": "warm_up",
            "location": 44,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "num_steps": [
                    "int",
                    "Optional[int]"
                ]
            }
        },
        "Runner.train": {
            "name": "train",
            "location": 56,
            "return": [
                "int",
                "Callable",
                "str"
            ],
            "arguments": {
                "self": [],
                "num_epochs": [
                    "int"
                ],
                "num_episodes": [
                    "int",
                    "float",
                    "Optional[float]",
                    "str",
                    "None"
                ],
                "render_frequency": [
                    "int",
                    "List[float]",
                    "float"
                ]
            }
        },
        "Runner.demonstrate": {
            "name": "demonstrate",
            "location": 69,
            "return": [
                "Tuple[(float, Any, Any, int)]"
            ],
            "arguments": {
                "self": [],
                "num_episodes": [
                    "int",
                    "Optional[int]",
                    "None"
                ]
            }
        },
        "Runner.render": {
            "name": "render",
            "location": 78,
            "return": [
                "Tuple[(Any, Any, int)]"
            ],
            "arguments": {
                "self": []
            }
        },
        "Runner.history": {
            "name": "history",
            "location": 84,
            "return": [
                "int",
                "Callable",
                "str"
            ],
            "arguments": {
                "self": []
            }
        },
        "Runner.run_episode": {
            "name": "run_episode",
            "location": 87,
            "return": [
                "Tuple[(Any, Any, int)]"
            ],
            "arguments": {
                "self": [],
                "epsilon": [
                    "float",
                    "int"
                ],
                "training": [
                    "bool",
                    "Optional[str]",
                    "int"
                ],
                "render": [
                    "bool",
                    "str",
                    "int",
                    "List"
                ]
            }
        },
        "Runner.run_epoch": {
            "name": "run_epoch",
            "location": 119,
            "return": [
                "Tuple[(float, list, List[int], int)]"
            ],
            "arguments": {
                "self": [],
                "epsilon": [
                    "List[str]",
                    "bytes",
                    "int"
                ],
                "num_episodes": [
                    "int",
                    "Tuple[float,float]",
                    "str"
                ],
                "training": [
                    "bool",
                    "List[str]",
                    "bytes",
                    "int"
                ],
                "render_frequency": [
                    "int",
                    "str",
                    "Tuple[float,float]"
                ]
            }
        }
    },
    "my_agents-master/my_agents/core/states.py": {
        "one_hot": {
            "name": "one_hot",
            "location": 3,
            "return": [
                "List[int]",
                "numpy.ndarray",
                "Iterable[str]",
                "Iterable[T]",
                "int"
            ],
            "arguments": {
                "size": [
                    "str",
                    "int",
                    "float"
                ],
                "idx": [
                    "float",
                    "int",
                    "str",
                    "bool"
                ]
            }
        },
        "StateSerializer.__init__": {
            "name": "__init__",
            "location": 16,
            "return": [
                "None"
            ],
            "arguments": {
                "self": [],
                "state_shape": [
                    "str",
                    "numpy.array",
                    "State[np.ndarray]"
                ]
            }
        },
        "StateSerializer.serialize": {
            "name": "serialize",
            "location": 20,
            "return": [
                "bool",
                "int",
                "Dict[str,Any]",
                "Dict"
            ],
            "arguments": {
                "self": [],
                "state": [
                    "int",
                    "numpy.ndarray",
                    "Dict[str,Any]",
                    "Dict"
                ]
            }
        },
        "StateSerializer.deserialize": {
            "name": "deserialize",
            "location": 25,
            "return": [
                "bool",
                "int",
                "Dict[str,Any]",
                "Dict"
            ],
            "arguments": {
                "self": [],
                "state": [
                    "int",
                    "numpy.ndarray",
                    "Dict[str,Any]",
                    "Dict"
                ]
            }
        },
        "StateSerializer.shape": {
            "name": "shape",
            "location": 31,
            "return": [
                "Callable",
                "List[str]",
                "dict",
                "int"
            ],
            "arguments": {
                "self": []
            }
        },
        "StateSerializer.from_num_states": {
            "name": "from_num_states",
            "location": 35,
            "return": [
                "StateSerializer"
            ],
            "arguments": {
                "num_states": [
                    "int",
                    "float"
                ]
            }
        }
    },
    "my_agents-master/my_agents/core/visualization.py": {
        "rolling_mean": {
            "name": "rolling_mean",
            "location": 12,
            "return": [
                "int",
                "List",
                "Callable",
                "Tuple[Union[Any,Any,Any,Any]]",
                "str"
            ],
            "arguments": {
                "history": [
                    "int",
                    "List[Dict]",
                    "list",
                    "List[str]",
                    "List[int]",
                    "bool"
                ],
                "window": [
                    "bool",
                    "int"
                ],
                "label": [
                    "str",
                    "None",
                    "Optional[Sequence[Any]]",
                    "Optional[int]",
                    "Sequence[Any]",
                    "int"
                ],
                "axis": [
                    "None",
                    "List[List[Callable]]",
                    "List",
                    "int",
                    "List[list[Callable]]",
                    "bool",
                    "Optional[int]"
                ],
                "show": [
                    "bool",
                    "int",
                    "List[str]",
                    "List[Tuple[int,int]]",
                    "float"
                ]
            }
        }
    }
}